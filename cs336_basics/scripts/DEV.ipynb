{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "153fa639",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da7993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import regex as re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from typing import BinaryIO\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f7a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "TOKEN = tuple[bytes]\n",
    "PAIR = tuple[bytes, bytes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f2e7f3",
   "metadata": {},
   "source": [
    "## 1 Assignment Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e287b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a80feac",
   "metadata": {},
   "source": [
    "## 2 Byte-Pair Encoding (BPE) Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcdbb3e",
   "metadata": {},
   "source": [
    "### 2.1 The Unicode Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9bf5df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29275, '牛']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord('牛'), chr(29275)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209ddd0",
   "metadata": {},
   "source": [
    "#### Problem (unicode1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d9697",
   "metadata": {},
   "source": [
    "##### a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b90bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f463d9",
   "metadata": {},
   "source": [
    "##### b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f9fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6078b",
   "metadata": {},
   "source": [
    "##### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96daa256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this is a test\" + chr(0) + \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86647719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885fe342",
   "metadata": {},
   "source": [
    "### 2.2 Unicode Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12077a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc24edd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "print(type(utf8_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c38939e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 33,\n",
       " 32,\n",
       " 227,\n",
       " 129,\n",
       " 147,\n",
       " 227,\n",
       " 130,\n",
       " 147,\n",
       " 227,\n",
       " 129,\n",
       " 171,\n",
       " 227,\n",
       " 129,\n",
       " 161,\n",
       " 227,\n",
       " 129,\n",
       " 175,\n",
       " 33]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(utf8_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3322f2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 23]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(test_string),len(utf8_encoded)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6bb20ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello! こんにちは!\n"
     ]
    }
   ],
   "source": [
    "print(utf8_encoded.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b18de75",
   "metadata": {},
   "source": [
    "#### Problem (unicode2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed709893",
   "metadata": {},
   "source": [
    "##### a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf37160",
   "metadata": {},
   "source": [
    "##### b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8cfb916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35a3b6be",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe4 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhello, 你好\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mdecode_utf8_bytes_to_str_wrong\u001b[39m\u001b[34m(bytestring)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xe4 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "decode_utf8_bytes_to_str_wrong(\"hello, 你好\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b68518c",
   "metadata": {},
   "source": [
    "##### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b92654d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'h',\n",
       " b'e',\n",
       " b'l',\n",
       " b'l',\n",
       " b'o',\n",
       " b',',\n",
       " b' ',\n",
       " b'\\xe4',\n",
       " b'\\xbd',\n",
       " b'\\xa0',\n",
       " b'\\xe5',\n",
       " b'\\xa5',\n",
       " b'\\xbd']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"hello, 你好\".encode(\"utf-8\")\n",
    "[bytes([b]) for b in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2713459a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(b'\\xe4\\xbd\\xa0').decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e6159",
   "metadata": {},
   "source": [
    "### 2.3 Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7babd",
   "metadata": {},
   "source": [
    "### 2.4 BPE Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1290735e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1de4c71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BA', 'A')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1afabb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"\"\"low low low low low lower lower widest widest widest newest newest newest newest newest newest\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f73644c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['low',\n",
       " ' low',\n",
       " ' low',\n",
       " ' low',\n",
       " ' low',\n",
       " ' lower',\n",
       " ' lower',\n",
       " ' widest',\n",
       " ' widest',\n",
       " ' widest',\n",
       " ' newest',\n",
       " ' newest',\n",
       " ' newest',\n",
       " ' newest',\n",
       " ' newest',\n",
       " ' newest']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(PAT, test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f545de",
   "metadata": {},
   "source": [
    "### 2.5 Experimenting with BPE Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5143d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO,\n",
    "    desired_num_chunks: int,\n",
    "    split_special_token: bytes,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), \"Must represent special token as a bytestring\"\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))\n",
    "\n",
    "def update_token_freqs(\n",
    "    token_freqs: dict[TOKEN, int],\n",
    "    text_segment: str,\n",
    "):\n",
    "    matches = re.finditer(PAT, text_segment)\n",
    "    for m in matches:\n",
    "        token = m.group()\n",
    "        token_bytes = tuple(bytes([b]) for b in token.encode(\"utf-8\"))\n",
    "        token_freqs[token_bytes] += 1 \n",
    "\n",
    "def process_chunk(args):\n",
    "    input_path, start, end, special_tokens, chunk_id = args\n",
    "    # Read chunk\n",
    "    with open(input_path, 'rb') as f:\n",
    "        f.seek(start)\n",
    "        chunk_bytes = f.read(end - start)\n",
    "    \n",
    "    # Decode\n",
    "    chunk_str = chunk_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
    "    \n",
    "    # Logic from original train_bpe\n",
    "    special_pat = \"|\".join(re.escape(st) for st in special_tokens)\n",
    "    segments = re.split(special_pat, chunk_str)\n",
    "    \n",
    "    token_freqs = defaultdict(int)\n",
    "    # Use position=chunk_id+1 so 0 is left for the main bar\n",
    "    for seg in segments:\n",
    "        update_token_freqs(seg, token_freqs)\n",
    "    \n",
    "    return token_freqs\n",
    "\n",
    "def get_pairs(\n",
    "    token: TOKEN\n",
    ") -> list[PAIR]:\n",
    "    if len(token) < 2:\n",
    "        return []\n",
    "    return [(token[i], token[i+1]) for i in range(len(token)-1)]\n",
    "\n",
    "def update_pair_freqs(\n",
    "    pair_freqs: dict[PAIR, int],\n",
    "    token_freqs: dict[TOKEN, int],\n",
    "):\n",
    "    for token, freq in token_freqs.items():\n",
    "        if len(token) < 2:\n",
    "            continue\n",
    "        pairs = get_pairs(token)\n",
    "        for p in pairs:\n",
    "            pair_freqs[p] += freq\n",
    "\n",
    "def update_pair2idx(\n",
    "    pair_to_idx: dict[PAIR, set[TOKEN]],\n",
    "    token_freqs: dict[TOKEN, int],\n",
    "    token_to_idx: dict[TOKEN, int],\n",
    "):\n",
    "    for token, freq in token_freqs.items():\n",
    "        if len(token) < 2:\n",
    "            continue\n",
    "        pairs = get_pairs(token)\n",
    "        for p in pairs:\n",
    "            pair_to_idx[p].add(token_to_idx[token])\n",
    "\n",
    "def update_token2pair(\n",
    "    token_freqs: dict[TOKEN, int],\n",
    "    token_to_pair: dict[TOKEN, list[PAIR]],\n",
    "):\n",
    "    for token, freq in token_freqs.items():\n",
    "        if len(token) < 2:\n",
    "            continue\n",
    "        pairs = get_pairs(token)\n",
    "        token_to_pair[token] = pairs\n",
    "\n",
    "def get_most_frequent_pair(\n",
    "    pair_freqs: dict[PAIR, int],\n",
    ") -> PAIR:\n",
    "    return max(pair_freqs.keys(), key=lambda k: (pair_freqs[k], k))\n",
    "\n",
    "def update_vocab(\n",
    "    new_id: int,\n",
    "    best_pair: PAIR,\n",
    "    vocab: dict[int, bytes],\n",
    "):\n",
    "    new_vocab = best_pair[0] + best_pair[1]\n",
    "    vocab[new_id] = new_vocab\n",
    "\n",
    "def update_vocab_inverse(\n",
    "    new_id: int,\n",
    "    best_pair: PAIR,\n",
    "    vocab_inverse: dict[bytes, int],\n",
    "):\n",
    "    new_vocab = best_pair[0] + best_pair[1]\n",
    "    vocab_inverse[new_vocab] = new_id\n",
    "\n",
    "def update_merges(\n",
    "    best_pair: PAIR,\n",
    "    merges: list[PAIR],\n",
    "):\n",
    "    merges.append(best_pair)\n",
    "\n",
    "def update_all(\n",
    "    best_pair: PAIR,\n",
    "    pair_to_token: dict[PAIR, set[TOKEN]],\n",
    "    token_to_pair: dict[TOKEN, list[PAIR]],\n",
    "    token_freqs: dict[TOKEN, int],\n",
    "    pair_freqs: dict[PAIR, int],\n",
    "):\n",
    "    affected_tokens = list(pair_to_token[best_pair])\n",
    "    merged_bytes = best_pair[0] + best_pair[1]\n",
    "\n",
    "    for token in affected_tokens:\n",
    "        # get new token\n",
    "        i=0\n",
    "        new_token = []\n",
    "        while(i<(len(token))):\n",
    "            if (i < len(token) - 1) and (token[i] == best_pair[0]) and (token[i+1] == best_pair[1]):\n",
    "                new_token.append(merged_bytes)\n",
    "                i = i + 2\n",
    "            else:\n",
    "                new_token.append(token[i])\n",
    "                i = i + 1\n",
    "        new_token = tuple(new_token)\n",
    "\n",
    "\n",
    "        ## update pair_to_token\n",
    "        new_pairs = get_pairs(new_token)\n",
    "        affected_pairs = token_to_pair[token]\n",
    "        for pair in affected_pairs:\n",
    "            pair_to_token[pair].discard(token)\n",
    "        for pair in new_pairs:\n",
    "            pair_to_token[pair].add(new_token)\n",
    "\n",
    "\n",
    "        ## update token_to_pair\n",
    "        token_to_pair.pop(token)\n",
    "        token_to_pair[new_token] = new_pairs\n",
    "\n",
    "\n",
    "        ## update pair_freqs\n",
    "        for pair in affected_pairs:\n",
    "            pair_freqs[pair] -= token_freqs[token]\n",
    "        for pair in new_pairs:\n",
    "            pair_freqs[pair] += token_freqs[token]\n",
    "\n",
    "\n",
    "        ## update token_freqs\n",
    "        origin_freq = token_freqs.pop(token)\n",
    "        token_freqs[new_token] = origin_freq\n",
    "        \n",
    "\n",
    "def train_bpe(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    special_tokens: list[str],\n",
    "    num_processes: int = 4\n",
    "):\n",
    "    print(\"Start Training BPE...\")\n",
    "    start_total = time.time()\n",
    "\n",
    "    print(\"Initialize Vocab and Merges...\")\n",
    "    vocab = {i:bytes([i]) for i in range(256)}\n",
    "    vocab_inverse = {v:k for k,v in vocab.items()}\n",
    "    # Add special tokens\n",
    "    for st in special_tokens:\n",
    "        new_id = len(vocab)\n",
    "        st_bytes = st.encode(\"utf-8\")\n",
    "        vocab[new_id] = st_bytes\n",
    "        vocab_inverse[st_bytes] = new_id\n",
    "    \n",
    "    merges = []\n",
    "\n",
    "    print(\"Calculating chunk boundaries...\")\n",
    "    t0 = time.time()\n",
    "    with open(input_path, 'rb') as f:\n",
    "        # Assuming first special token is the split token\n",
    "        split_token = special_tokens[0].encode(\"utf-8\") if special_tokens else b\"<|endoftext|>\"\n",
    "        boundaries = find_chunk_boundaries(f, num_processes, split_token) # More chunks than processes for load balancing\n",
    "    print(f\"Boundaries calculated in {time.time() - t0:.2f}s\")\n",
    "    \n",
    "    token_freqs = defaultdict(int)\n",
    "\n",
    "    ## Initiate token_freqs\n",
    "    print(\"Update Token Freq (Parallel)...\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    tasks = []\n",
    "    for i in range(len(boundaries) - 1):\n",
    "        start = boundaries[i]\n",
    "        end = boundaries[i+1]\n",
    "        tasks.append((input_path, start, end, special_tokens, i))\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
    "        results = list(tqdm(executor.map(process_chunk, tasks), total=len(tasks), desc=\"Token Freqs (Chunks)\", position=0))\n",
    "        \n",
    "        for local_freqs in results:\n",
    "            for token, count in local_freqs.items():\n",
    "                token_freqs[token] += count\n",
    "                \n",
    "    print(f\"Update Token Freq took {time.time() - t0:.2f}s\")\n",
    "\n",
    "    pair_freqs = defaultdict(int)\n",
    "    pair_to_token = defaultdict(set)\n",
    "    token_to_pair = defaultdict(list)\n",
    "\n",
    "    ## Initiate pair_freqs, pair_to_token, and token_to_pair\n",
    "    print(\"Update Pair Freq...\")\n",
    "    t0 = time.time()\n",
    "    update_pair_freqs(token_freqs, pair_freqs)\n",
    "    print(f\"Update Pair Freq took {time.time() - t0:.2f}s\")\n",
    "\n",
    "    print(\"Update Pair to Token...\")\n",
    "    t0 = time.time()\n",
    "    update_pair2token(token_freqs, pair_to_token)\n",
    "    print(f\"Update Pair to Token took {time.time() - t0:.2f}s\")\n",
    "\n",
    "    print(\"Update Token to Pair...\")\n",
    "    t0 = time.time()\n",
    "    update_token2pair(token_freqs, token_to_pair)\n",
    "    print(f\"Update Token to Pair took {time.time() - t0:.2f}s\")\n",
    "\n",
    "    print(\"Start Merging...\")\n",
    "    t0 = time.time()\n",
    "    num_merges = vocab_size - 256 - len(special_tokens)\n",
    "    for i in tqdm(range(num_merges), desc=\"Merging\"):\n",
    "        ### find most frequent pair\n",
    "        if not pair_freqs:\n",
    "            break\n",
    "        best_pair = get_most_frequent_pair(pair_freqs)\n",
    "\n",
    "        new_id = len(vocab)\n",
    "\n",
    "        ## update vocab\n",
    "        update_vocab(new_id, best_pair, vocab)\n",
    "\n",
    "        ## update vocab_inverse\n",
    "        update_vocab_inverse(new_id, best_pair, vocab_inverse)\n",
    "\n",
    "        ## update merges\n",
    "        update_merges(best_pair, merges)\n",
    "\n",
    "        update_all(best_pair, pair_to_token, token_to_pair, token_freqs, pair_freqs)\n",
    "    print(f\"Merging took {time.time() - t0:.2f}s\")\n",
    "    print(f\"Total Training took {time.time() - start_total:.2f}s\")\n",
    "\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71344ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"abere ererea<|endoftext|>When and where is not as important as who and what. Hi, I am the the Ivan.<|endoftext|> aaa\"\n",
    "# test_string = \"abere ererea\"\n",
    "input_path = \"\"\n",
    "vocab_size = 260\n",
    "special_tokens = ['<|endoftext|>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80e25f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abere ererea'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_pat = \"|\".join(re.escape(st) for st in special_tokens)\n",
    "segments = re.split(special_pat, test_string)\n",
    "\n",
    "text_segment = segments[0]\n",
    "text_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cf7e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {i:bytes([i]) for i in range(256)}\n",
    "vocab_inverse = {v:k for k,v in vocab.items()}\n",
    "# Add special tokens\n",
    "for st in special_tokens:\n",
    "    new_id = len(vocab)\n",
    "    st_bytes = st.encode(\"utf-8\")\n",
    "    vocab[new_id] = st_bytes\n",
    "    vocab_inverse[st_bytes] = new_id\n",
    "\n",
    "merges = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee804c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(b'a', b'b', b'e', b'r', b'e'): 1,\n",
       "             (b' ', b'e', b'r', b'e', b'r', b'e', b'a'): 1})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freqs = defaultdict(int)\n",
    "update_token_freqs(token_freqs,text_segment)\n",
    "token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91d25822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(b'a', b'b', b'e', b'r', b'e'): 0,\n",
       " (b' ', b'e', b'r', b'e', b'r', b'e', b'a'): 1}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_token = {i: k for i, k in enumerate(token_freqs.keys())}\n",
    "token_to_idx = {k: i for i, k in enumerate(token_freqs.keys())}\n",
    "token_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "824040e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(b'a', b'b'): 1,\n",
       "             (b'b', b'e'): 1,\n",
       "             (b'e', b'r'): 3,\n",
       "             (b'r', b'e'): 3,\n",
       "             (b' ', b'e'): 1,\n",
       "             (b'e', b'a'): 1})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_freqs = defaultdict(int)\n",
    "update_pair_freqs(pair_freqs,token_freqs)\n",
    "pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "892a9535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {(b'a', b'b'): {0},\n",
       "             (b'b', b'e'): {0},\n",
       "             (b'e', b'r'): {0, 1},\n",
       "             (b'r', b'e'): {0, 1},\n",
       "             (b' ', b'e'): {1},\n",
       "             (b'e', b'a'): {1}})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_to_idx = defaultdict(set)\n",
    "update_pair2idx(pair_to_idx, token_freqs, token_to_idx)\n",
    "pair_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99c45417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'r', b'e')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pair = get_most_frequent_pair(pair_freqs)\n",
    "best_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d55fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_id = len(vocab)\n",
    "\n",
    "## update vocab\n",
    "update_vocab(new_id, best_pair, vocab)\n",
    "\n",
    "## update vocab_inverse\n",
    "update_vocab_inverse(new_id, best_pair, vocab_inverse)\n",
    "\n",
    "## update merges\n",
    "update_merges(best_pair, merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9a1fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "affected_idxs = list(pair_to_idx[best_pair])\n",
    "merged_bytes = best_pair[0] + best_pair[1]\n",
    "for idx in affected_idxs:\n",
    "    token = idx_to_token[idx]\n",
    "\n",
    "    i=0\n",
    "    new_token = []\n",
    "    while(i<(len(token))):\n",
    "        if (i < len(token) - 1) and (token[i] == best_pair[0]) and (token[i+1] == best_pair[1]):\n",
    "            new_token.append(merged_bytes)\n",
    "            i = i + 2\n",
    "        else:\n",
    "            new_token.append(token[i])\n",
    "            i = i + 1\n",
    "    new_token = tuple(new_token)\n",
    "\n",
    "    new_pairs = get_pairs(new_token)\n",
    "    affected_pairs = get_pairs(token)\n",
    "    for p in set(new_pairs)-set(affected_pairs):\n",
    "        pair_to_idx[p].add(idx)\n",
    "    for p in set(affected_pairs)-set(new_pairs):\n",
    "        pair_to_idx[p].discard(idx)\n",
    "\n",
    "\n",
    "    for pair in affected_pairs:\n",
    "        pair_freqs[pair] -= token_freqs[token]\n",
    "    for pair in new_pairs:\n",
    "        pair_freqs[pair] += token_freqs[token]\n",
    "\n",
    "    origin_freq = token_freqs.pop(token)\n",
    "    token_freqs[new_token] = origin_freq\n",
    "\n",
    "    idx_to_token[idx] = new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6622a581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {(b'a', b'b'): {0},\n",
       "             (b'b', b'e'): {0},\n",
       "             (b'e', b'r'): set(),\n",
       "             (b'r', b'e'): set(),\n",
       "             (b' ', b'e'): {1},\n",
       "             (b'e', b'a'): set(),\n",
       "             (b'e', b're'): {0, 1},\n",
       "             (b're', b'a'): {1},\n",
       "             (b're', b're'): {1}})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c8bc38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(b'a', b'b'): 1,\n",
       "             (b'b', b'e'): 1,\n",
       "             (b'e', b'r'): 0,\n",
       "             (b'r', b'e'): 0,\n",
       "             (b' ', b'e'): 1,\n",
       "             (b'e', b'a'): 0,\n",
       "             (b'e', b're'): 2,\n",
       "             (b're', b're'): 1,\n",
       "             (b're', b'a'): 1})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4f47552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(b'a', b'b', b'e', b're'): 1,\n",
       "             (b' ', b'e', b're', b're', b'a'): 1})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bcb800c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (b'a', b'b', b'e', b're'), 1: (b' ', b'e', b're', b're', b'a')}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "980af86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'a', b'b', b'e', b'r', b'e')\n"
     ]
    }
   ],
   "source": [
    "affected_idxs = list(pair_to_idx[best_pair])\n",
    "merged_bytes = best_pair[0] + best_pair[1]\n",
    "idx = affected_idxs[0]\n",
    "token = idx_to_token[idx]\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f387281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'a', b'b', b'e', b're')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "new_token = []\n",
    "while(i<(len(token))):\n",
    "    if (i < len(token) - 1) and (token[i] == best_pair[0]) and (token[i+1] == best_pair[1]):\n",
    "        new_token.append(merged_bytes)\n",
    "        i = i + 2\n",
    "    else:\n",
    "        new_token.append(token[i])\n",
    "        i = i + 1\n",
    "new_token = tuple(new_token)\n",
    "new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01c34a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {(b'a', b'b'): {0},\n",
       "             (b'b', b'e'): {0},\n",
       "             (b'e', b'r'): {1},\n",
       "             (b'r', b'e'): {1},\n",
       "             (b' ', b'e'): {1},\n",
       "             (b'e', b'a'): {1},\n",
       "             (b'e', b're'): {0}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pairs = get_pairs(new_token)\n",
    "affected_pairs = get_pairs(token)\n",
    "for p in set(new_pairs)-set(affected_pairs):\n",
    "    pair_to_idx[p].add(idx)\n",
    "for p in set(affected_pairs)-set(new_pairs):\n",
    "    pair_to_idx[p].discard(idx)\n",
    "pair_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0f4766f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(b'a', b'b'): 1,\n",
       "             (b'b', b'e'): 1,\n",
       "             (b'e', b'r'): 2,\n",
       "             (b'r', b'e'): 2,\n",
       "             (b' ', b'e'): 1,\n",
       "             (b'e', b'a'): 1,\n",
       "             (b'e', b're'): 1})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for pair in affected_pairs:\n",
    "    pair_freqs[pair] -= token_freqs[token]\n",
    "for pair in new_pairs:\n",
    "    pair_freqs[pair] += token_freqs[token]\n",
    "pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d2243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(b'e', b'r'), (b'r', b'e')}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## update token_freqs\n",
    "origin_freq = token_freqs.pop(token)\n",
    "token_freqs[new_token] = origin_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ace831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(b'a', b'b', b'e', b'r', b'e'): 1,\n",
       "             (b' ', b'e', b'r', b'e', b'r', b'e', b'a'): 1})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## update idx2token\n",
    "idx_to_token[idx] = new_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d83e3bd",
   "metadata": {},
   "source": [
    "- [] vocab\n",
    "- [] vocab_inverse \n",
    "- [] merges \n",
    "- [] tokens\n",
    "- [] token_freqs\n",
    "- [] pair_freqs\n",
    "- [] pair_to_token\n",
    "- [] token_to_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "610a05f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_id = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1611cd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abere ererea'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_pat = \"|\".join(re.escape(st) for st in special_tokens)\n",
    "segments = re.split(special_pat, test_string)\n",
    "text_segment = segments[0]\n",
    "text_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6066514b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96a7c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freqs = defaultdict(int)\n",
    "pair_freqs = defaultdict(int)\n",
    "pair_to_token = defaultdict(set)\n",
    "token_to_pair = defaultdict(list)\n",
    "\n",
    "## Initiate token_freqs\n",
    "token_freqs = update_token_freqs(text_segment, token_freqs)\n",
    "\n",
    "# Initiate pair_freqs, pair_to_token, and token_to_pair\n",
    "pair_freqs = update_pair_freqs(token_freqs, pair_freqs)\n",
    "pair_to_token = update_pair2token(token_freqs, pair_to_token)\n",
    "token_to_pair = update_token2pair(token_freqs, token_to_pair)\n",
    "\n",
    "\n",
    "### find most frequent pair\n",
    "best_pair = get_most_frequent_pair(pair_freqs)\n",
    "\n",
    "## update vocab\n",
    "vocab = update_vocab(new_id, best_pair, vocab)\n",
    "\n",
    "## update vocab_inverse\n",
    "vocab_inverse = update_vocab_inverse(new_id, best_pair, vocab_inverse)\n",
    "\n",
    "## update merges\n",
    "merges = update_merges(best_pair, merges)\n",
    "\n",
    "\n",
    "pair_freqs, pair_to_token, token_to_pair, token_freqs = update_all(best_pair, pair_freqs, pair_to_token, token_to_pair, token_freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e9fc0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(b'a', b'b'): 1,\n",
       "             (b'b', b'e'): 1,\n",
       "             (b'e', b'r'): 0,\n",
       "             (b'r', b'e'): 0,\n",
       "             (b' ', b'e'): 1,\n",
       "             (b'e', b'a'): 0,\n",
       "             (b'e', b're'): 2,\n",
       "             (b're', b're'): 1,\n",
       "             (b're', b'a'): 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "523f249a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {(b'a', b'b'): {(b'a', b'b', b'e', b'r', b'e')},\n",
       "             (b'b', b'e'): {(b'a', b'b', b'e', b'r', b'e')},\n",
       "             (b'e', b'r'): {(b' ', b'e', b'r', b'e', b'r', b'e', b'a'),\n",
       "              (b'a', b'b', b'e', b'r', b'e')},\n",
       "             (b'r', b'e'): {(b' ', b'e', b'r', b'e', b'r', b'e', b'a'),\n",
       "              (b'a', b'b', b'e', b'r', b'e')},\n",
       "             (b' ', b'e'): {(b' ', b'e', b'r', b'e', b'r', b'e', b'a')},\n",
       "             (b'e', b'a'): {(b' ', b'e', b'r', b'e', b'r', b'e', b'a')}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7feb0034",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m affected_tokens = \u001b[38;5;28mlist\u001b[39m(pair_to_token[best_pair])\n\u001b[32m      2\u001b[39m merged_bytes = best_pair[\u001b[32m0\u001b[39m] + best_pair[\u001b[32m1\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m token = \u001b[43maffected_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      4\u001b[39m token\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "affected_tokens = list(pair_to_token[best_pair])\n",
    "merged_bytes = best_pair[0] + best_pair[1]\n",
    "token = affected_tokens[0]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d488de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "new_token = []\n",
    "while(i<(len(token))):\n",
    "    if (i < len(token) - 1) and (token[i] == best_pair[0]) and (token[i+1] == best_pair[1]):\n",
    "        new_token.append(merged_bytes)\n",
    "        i = i + 2\n",
    "    else:\n",
    "        new_token.append(token[i])\n",
    "        i = i + 1\n",
    "new_token = tuple(new_token)\n",
    "\n",
    "\n",
    "## update pair_to_token\n",
    "new_pairs = [(new_token[i], new_token[i+1]) for i in range(len(new_token)-1)]\n",
    "affected_pairs = token_to_pair[token]\n",
    "for pair in affected_pairs:\n",
    "    pair_to_token[pair].discard(token)\n",
    "for pair in new_pairs:\n",
    "    pair_to_token[pair].add(new_token)\n",
    "\n",
    "\n",
    "## update token_to_pair\n",
    "token_to_pair.pop(token)\n",
    "token_to_pair[new_token] = new_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88fe5fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'a', b'b'), (b'b', b'e'), (b'e', b're')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad1899",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Set changed size during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[135]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m affected_tokens = pair_to_token[best_pair]\n\u001b[32m      2\u001b[39m merged_bytes = best_pair[\u001b[32m0\u001b[39m] + best_pair[\u001b[32m1\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43maffected_tokens\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# get new token\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_token\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Set changed size during iteration"
     ]
    }
   ],
   "source": [
    "affected_tokens = list(pair_to_token[best_pair])\n",
    "merged_bytes = best_pair[0] + best_pair[1]\n",
    "\n",
    "for token in list(affected_tokens):\n",
    "    # get new token\n",
    "    i=0\n",
    "    new_token = []\n",
    "    while(i<(len(token))):\n",
    "        if (i < len(token) - 1) and (token[i] == best_pair[0]) and (token[i+1] == best_pair[1]):\n",
    "            new_token.append(merged_bytes)\n",
    "            i = i + 2\n",
    "        else:\n",
    "            new_token.append(token[i])\n",
    "            i = i + 1\n",
    "    new_token = tuple(new_token)\n",
    "\n",
    "\n",
    "    ## update pair_to_token\n",
    "    new_pairs = [(new_token[i], new_token[i+1]) for i in range(len(new_token)-1)]\n",
    "    affected_pairs = token_to_pair[token]\n",
    "    for pair in affected_pairs:\n",
    "        pair_to_token[pair].discard(token)\n",
    "    for pair in new_pairs:\n",
    "        pair_to_token[pair].add(new_token)\n",
    "\n",
    "\n",
    "    ## update token_to_pair\n",
    "    token_to_pair.pop(token)\n",
    "    token_to_pair[new_token] = new_pairs\n",
    "\n",
    "\n",
    "    ## update pair_freqs\n",
    "    for pair in affected_pairs:\n",
    "        pair_freqs[pair] -= token_freqs[token]\n",
    "    for pair in new_pairs:\n",
    "        pair_freqs[pair] += token_freqs[token]\n",
    "\n",
    "\n",
    "    ## update token_freqs\n",
    "    origin_freq = token_freqs.pop(token)\n",
    "    token_freqs[new_token] = origin_freq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5016a159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(b'a', b'b', b'e', b're'): 1,\n",
       "             (b' ', b'e', b're', b're', b'a'): 1})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b7746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b' ', b'e', b'r', b'e', b'r', b'e', b'a')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## update affected_tokens\n",
    "affected_tokens = pair_to_token[best_pair]\n",
    "merged_bytes = best_pair[0] + best_pair[1]\n",
    "\n",
    "token = (next(iter(affected_tokens)))\n",
    "token = (b' ', b'e', b'r', b'e', b'r', b'e', b'a')\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ffb3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b' ', b'e', b're', b're', b'a')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "new_token = []\n",
    "while(i<(len(token))):\n",
    "    if (token[i] == best_pair[0]) and (token[i+1] == best_pair[1]):\n",
    "        new_token.append(merged_bytes)\n",
    "        i = i + 2\n",
    "    else:\n",
    "        new_token.append(token[i])\n",
    "        i = i + 1\n",
    "new_token = tuple(new_token)\n",
    "new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "28f74ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {(b'a', b'b'): {(b'a', b'b', b'e', b'r', b'e')},\n",
       "             (b'b', b'e'): {(b'a', b'b', b'e', b'r', b'e')},\n",
       "             (b'e', b'r'): {(b'a', b'b', b'e', b'r', b'e')},\n",
       "             (b'r', b'e'): {(b'a', b'b', b'e', b'r', b'e')},\n",
       "             (b' ', b'e'): {(b' ', b'e', b're', b're', b'a')},\n",
       "             (b'e', b'a'): set(),\n",
       "             (b'e', b're'): {(b' ', b'e', b're', b're', b'a')},\n",
       "             (b're', b're'): {(b' ', b'e', b're', b're', b'a')},\n",
       "             (b're', b'a'): {(b' ', b'e', b're', b're', b'a')}})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## update pair_to_token\n",
    "new_pairs = [(new_token[i], new_token[i+1]) for i in range(len(new_token)-1)]\n",
    "affected_pairs = token_to_pair[token]\n",
    "for pair in affected_pairs:\n",
    "    pair_to_token[pair].discard(token)\n",
    "for pair in new_pairs:\n",
    "    pair_to_token[pair].add(new_token)\n",
    "pair_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "056951f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(b' ', b'e', b're', b're', b'a')}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_to_token[pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34576879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {(b'a', b'b', b'e', b'r', b'e'): {(b'a', b'b'),\n",
       "              (b'b', b'e'),\n",
       "              (b'e', b'r'),\n",
       "              (b'r', b'e')},\n",
       "             (b' ', b'e', b're', b're', b'a'): [(b' ', b'e'),\n",
       "              (b'e', b're'),\n",
       "              (b're', b're'),\n",
       "              (b're', b'a')]})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## update token_to_pair\n",
    "token_to_pair.pop(token)\n",
    "token_to_pair[new_token] = new_pairs\n",
    "token_to_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c35170da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(b'a', b'b'): 1,\n",
       "             (b'b', b'e'): 1,\n",
       "             (b'e', b'r'): 3,\n",
       "             (b'r', b'e'): 3,\n",
       "             (b' ', b'e'): 1,\n",
       "             (b'e', b'a'): 1})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2dccbbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "affected_pairs = [(token[i], token[i+1]) for i in range(len(token)-1)]\n",
    "new_pairs = [(new_token[i], new_token[i+1]) for i in range(len(new_token)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ecadbb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(b'a', b'b'): 1,\n",
       "             (b'b', b'e'): 1,\n",
       "             (b'e', b'r'): 1,\n",
       "             (b'r', b'e'): 1,\n",
       "             (b' ', b'e'): 1,\n",
       "             (b'e', b'a'): 0,\n",
       "             (b'e', b're'): 1,\n",
       "             (b're', b're'): 1,\n",
       "             (b're', b'a'): 1})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## update pair_freqs\n",
    "for pair in affected_pairs:\n",
    "    pair_freqs[pair] -= token_freqs[token]\n",
    "for pair in new_pairs:\n",
    "    pair_freqs[pair] += token_freqs[token]\n",
    "pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c1fe7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {(b'a', b'b', b'e', b'r', b'e'): 1, (b' ',): 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b9da876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {(b' ',): 1, (b'a', b'b', b'e', b're'): 1})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## update token_freqs\n",
    "origin_freq = token_freqs.pop(token)\n",
    "token_freqs[new_token] = origin_freq\n",
    "token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39222fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(b'e', b're')], 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "135ee375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(b'a', b'b'): 1,\n",
       "             (b'b', b'e'): 1,\n",
       "             (b'e', b'r'): 5,\n",
       "             (b'r', b'e'): 4,\n",
       "             (b' ', b'e'): 1,\n",
       "             (b'e', b'a'): 1,\n",
       "             (b'e', b're'): 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_freqs[best_pair] -= duplications_count*token_freqs[new_token]\n",
    "for n in new_neighbors:\n",
    "    pair_freqs[n] += token_freqs[new_token]\n",
    "pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3521672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {(b'a', b'b'): {(b'a', b'b', b'e', b'r', b'e')}, (b'b', b'e'): {(b'a', b'b', b'e', b'r', b'e')}, (b'e', b'r'): {(b'a', b'b', b'e', b'r', b'e'), (b' ', b'e', b'r', b'e', b'r', b'e', b'r', b'e', b'r', b'e', b'a')}, (b'r', b'e'): {(b'a', b'b', b'e', b'r', b'e')}, (b' ', b'e'): {(b' ', b'e', b'r', b'e', b'r', b'e', b'r', b'e', b'r', b'e', b'a')}, (b'e', b'a'): {(b' ', b'e', b'r', b'e', b'r', b'e', b'r', b'e', b'r', b'e', b'a')}, (b'e', b're'): {(b' ', b'e', b're', b're', b're', b're', b'a')}, (b're', b're'): {(b' ', b'e', b're', b're', b're', b're', b'a')}, (b're', b'a'): {(b' ', b'e', b're', b're', b're', b're', b'a')}})\n"
     ]
    }
   ],
   "source": [
    "## update pair_to_token and token_to_pair\n",
    "pair_to_token[best_pair].discard(token)\n",
    "for n in new_neighbors:\n",
    "    pair_to_token[n].add(new_token)\n",
    "print(pair_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "263ee4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {(b'a', b'b', b'e', b'r', b'e'): {(b'a', b'b'),\n",
       "              (b'b', b'e'),\n",
       "              (b'e', b'r'),\n",
       "              (b'r', b'e')},\n",
       "             (b' ',\n",
       "              b'e',\n",
       "              b'r',\n",
       "              b'e',\n",
       "              b'r',\n",
       "              b'e',\n",
       "              b'r',\n",
       "              b'e',\n",
       "              b'r',\n",
       "              b'e',\n",
       "              b'a'): {(b' ', b'e'), (b'e', b'a'), (b'e', b'r'), (b'r', b'e')}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9710eea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'a', b'b', b'e', b're']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d4118cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'r', b'e')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bc7a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "## update pair_freqs and pair_to_token\n",
    "affected_tokens = pair_to_token[best_pair]\n",
    "merged_bytes = best_pair[0]+best_pair[1]\n",
    "for token in affected_tokens:\n",
    "    new_neighbors = []\n",
    "    i=0\n",
    "    duplications_count = 0\n",
    "    while(i<len(token)-len(merged_bytes)+1):\n",
    "        if token[i:(i+len(merged_bytes))] == merged_bytes:\n",
    "            duplications_count += 1\n",
    "            if i != 0:\n",
    "                new_neighbors.append((bytes([token[i-1]]), merged_bytes))\n",
    "            if i + len(merged_bytes) != len(token):\n",
    "                new_neighbors.append((merged_bytes, bytes([token[i + len(merged_bytes)]])))\n",
    "            i += len(merged_bytes)\n",
    "        else:\n",
    "            i += 1\n",
    "    pair_freqs[best_pair] -= duplications_count*word_freqs[token]\n",
    "    for pair in new_neighbors:\n",
    "        pair_freqs[pair] += word_freqs[token]\n",
    "        pair_to_token[pair].add(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948772ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(b'a', b'b'): 1,\n",
       "             (b'b', b'e'): 1,\n",
       "             (b'e', b'r'): 5,\n",
       "             (b'r', b'e'): 0,\n",
       "             (b' ', b'e'): 1,\n",
       "             (b'e', b're'): 5,\n",
       "             (b're', b'r'): 3})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5174d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b' erererere', b'abere'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affected_token = next(iter(affected_tokens))\n",
    "\n",
    "duplications_count = 0\n",
    "while(i<len(affected_token)-len(merged_bytes)+1):\n",
    "    print(i)\n",
    "    if affected_token[i:(i+len(merged_bytes))] == merged_bytes:\n",
    "        duplications_count += 1\n",
    "        if i != 0:\n",
    "            new_neighbors.append((bytes([affected_token[i-1]]), merged_bytes))\n",
    "        if i + len(merged_bytes) != len(affected_token):\n",
    "            new_neighbors.append((merged_bytes, bytes([affected_token[i + len(merged_bytes)]])))\n",
    "        i += len(merged_bytes)\n",
    "    else:\n",
    "        i += 1\n",
    "new_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f82a057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplications_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_freqs[best_pair] -= duplications_count*word_freqs[affected_token]\n",
    "for pair in new_neighbors:\n",
    "    pair_freqs[pair] += word_freqs[affected_token]\n",
    "    pair_to_token[pair].add(affected_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e522893e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {(b'a', b'b'): {b'abere'},\n",
       "             (b'b', b'e'): {b'abere'},\n",
       "             (b'e', b'r'): {b' erererere', b'abere'},\n",
       "             (b'r', b'e'): {b' erererere', b'abere'},\n",
       "             (b' ', b'e'): {b' erererere'},\n",
       "             (b'e', b're'): {b' erererere'},\n",
       "             (b're', b'r'): {b' erererere'}})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c197d5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(b'a', b'b'): 1,\n",
       "             (b'b', b'e'): 1,\n",
       "             (b'e', b'r'): 5,\n",
       "             (b'r', b'e'): 1,\n",
       "             (b' ', b'e'): 1,\n",
       "             (b'e', b're'): 4,\n",
       "             (b're', b'r'): 3})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa760e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
