{
   "cells": [
      {
         "cell_type": "markdown",
         "id": "153fa639",
         "metadata": {},
         "source": [
            "## Start"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 22,
         "id": "2da7993a",
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "import pickle\n",
            "import time\n",
            "import random\n",
            "import torch\n",
            "\n",
            "\n",
            "import regex as re\n",
            "import numpy as np\n",
            "import torch.nn as nn\n",
            "\n",
            "from tqdm import tqdm\n",
            "from concurrent.futures import ProcessPoolExecutor\n",
            "from typing import BinaryIO\n",
            "from collections import defaultdict\n",
            "from collections.abc import Iterable, Iterator\n",
            "from einops import rearrange, einsum"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "id": "f8f7a596",
         "metadata": {},
         "outputs": [],
         "source": [
            "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
            "TOKEN = tuple[bytes]\n",
            "PAIR = tuple[bytes, bytes]\n",
            "END_TOKEN = '<|endoftext|>'"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "id": "48a575f8",
         "metadata": {},
         "outputs": [],
         "source": [
            "import sys\n",
            "import os\n",
            "# 获取当前 notebook 所在目录的上两级目录（即项目根目录）\n",
            "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
            "# 将项目根目录加入 sys.path\n",
            "if project_root not in sys.path:\n",
            "    sys.path.append(project_root)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "a2f2e7f3",
         "metadata": {},
         "source": [
            "## 1 Assignment Overview"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "f54e287b",
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "markdown",
         "id": "2a80feac",
         "metadata": {},
         "source": [
            "## 2 Byte-Pair Encoding (BPE) Tokenizer"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "6fcdbb3e",
         "metadata": {},
         "source": [
            "### 2.1 The Unicode Standard"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "id": "c9bf5df8",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[29275, '牛']"
                  ]
               },
               "execution_count": 3,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "[ord('牛'), chr(29275)]"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "a209ddd0",
         "metadata": {},
         "source": [
            "#### Problem (unicode1)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "bb1d9697",
         "metadata": {},
         "source": [
            "##### a"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "45b90bd1",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'\\x00'"
                  ]
               },
               "execution_count": 3,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "chr(0)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "46f463d9",
         "metadata": {},
         "source": [
            "##### b"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "c91f9fdb",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\u0000\n"
               ]
            }
         ],
         "source": [
            "print(chr(0))"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "01b6078b",
         "metadata": {},
         "source": [
            "##### c"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "id": "96daa256",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'this is a test\\x00string'"
                  ]
               },
               "execution_count": 8,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "\"this is a test\" + chr(0) + \"string\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "id": "86647719",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "this is a test\u0000string\n"
               ]
            }
         ],
         "source": [
            "print(\"this is a test\" + chr(0) + \"string\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "885fe342",
         "metadata": {},
         "source": [
            "### 2.2 Unicode Encodings"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "id": "12077a57",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n"
               ]
            }
         ],
         "source": [
            "test_string = \"hello! こんにちは!\"\n",
            "utf8_encoded = test_string.encode(\"utf-8\")\n",
            "print(utf8_encoded)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "id": "dc24edd9",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "<class 'bytes'>\n"
               ]
            }
         ],
         "source": [
            "print(type(utf8_encoded))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "id": "7c38939e",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[104,\n",
                     " 101,\n",
                     " 108,\n",
                     " 108,\n",
                     " 111,\n",
                     " 33,\n",
                     " 32,\n",
                     " 227,\n",
                     " 129,\n",
                     " 147,\n",
                     " 227,\n",
                     " 130,\n",
                     " 147,\n",
                     " 227,\n",
                     " 129,\n",
                     " 171,\n",
                     " 227,\n",
                     " 129,\n",
                     " 161,\n",
                     " 227,\n",
                     " 129,\n",
                     " 175,\n",
                     " 33]"
                  ]
               },
               "execution_count": 12,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "list(utf8_encoded)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "id": "3322f2e7",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[13, 23]"
                  ]
               },
               "execution_count": 13,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "[len(test_string),len(utf8_encoded)]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "id": "b6bb20ca",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "hello! こんにちは!\n"
               ]
            }
         ],
         "source": [
            "print(utf8_encoded.decode(\"utf-8\"))"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "0b18de75",
         "metadata": {},
         "source": [
            "#### Problem (unicode2)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "ed709893",
         "metadata": {},
         "source": [
            "##### a"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "faf37160",
         "metadata": {},
         "source": [
            "##### b"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "id": "f8cfb916",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'hello'"
                  ]
               },
               "execution_count": 15,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
            "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
            "decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "id": "35a3b6be",
         "metadata": {},
         "outputs": [
            {
               "ename": "UnicodeDecodeError",
               "evalue": "'utf-8' codec can't decode byte 0xe4 in position 0: unexpected end of data",
               "output_type": "error",
               "traceback": [
                  "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                  "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
                  "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhello, 你好\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
                  "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mdecode_utf8_bytes_to_str_wrong\u001b[39m\u001b[34m(bytestring)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
                  "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xe4 in position 0: unexpected end of data"
               ]
            }
         ],
         "source": [
            "decode_utf8_bytes_to_str_wrong(\"hello, 你好\".encode(\"utf-8\"))"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "8b68518c",
         "metadata": {},
         "source": [
            "##### c"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "id": "5b92654d",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[b'h',\n",
                     " b'e',\n",
                     " b'l',\n",
                     " b'l',\n",
                     " b'o',\n",
                     " b',',\n",
                     " b' ',\n",
                     " b'\\xe4',\n",
                     " b'\\xbd',\n",
                     " b'\\xa0',\n",
                     " b'\\xe5',\n",
                     " b'\\xa5',\n",
                     " b'\\xbd']"
                  ]
               },
               "execution_count": 18,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "sentence = \"hello, 你好\".encode(\"utf-8\")\n",
            "[bytes([b]) for b in sentence]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 24,
         "id": "2713459a",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'你'"
                  ]
               },
               "execution_count": 24,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "(b'\\xe4\\xbd\\xa0').decode(\"utf-8\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "259e6159",
         "metadata": {},
         "source": [
            "### 2.3 Subword Tokenization"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "5eb7babd",
         "metadata": {},
         "source": [
            "### 2.4 BPE Tokenizer Training"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 27,
         "id": "1290735e",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
                  ]
               },
               "execution_count": 27,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "re.findall(PAT, \"some text that i'll pre-tokenize\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 28,
         "id": "1de4c71d",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "('BA', 'A')"
                  ]
               },
               "execution_count": 28,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "max([(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 31,
         "id": "1afabb34",
         "metadata": {},
         "outputs": [],
         "source": [
            "test_string = \"\"\"low low low low low lower lower widest widest widest newest newest newest newest newest newest\"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 32,
         "id": "6f73644c",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "['low',\n",
                     " ' low',\n",
                     " ' low',\n",
                     " ' low',\n",
                     " ' low',\n",
                     " ' lower',\n",
                     " ' lower',\n",
                     " ' widest',\n",
                     " ' widest',\n",
                     " ' widest',\n",
                     " ' newest',\n",
                     " ' newest',\n",
                     " ' newest',\n",
                     " ' newest',\n",
                     " ' newest',\n",
                     " ' newest']"
                  ]
               },
               "execution_count": 32,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "re.findall(PAT, test_string)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "30f545de",
         "metadata": {},
         "source": [
            "### 2.5 Experimenting with BPE Tokenizer Training"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "id": "5143d682",
         "metadata": {},
         "outputs": [],
         "source": [
            "def find_chunk_boundaries(\n",
            "    file: BinaryIO,\n",
            "    desired_num_chunks: int,\n",
            "    split_special_token: bytes,\n",
            ") -> list[int]:\n",
            "    \"\"\"\n",
            "    Chunk the file into parts that can be counted independently.\n",
            "    May return fewer chunks if the boundaries end up overlapping.\n",
            "    \"\"\"\n",
            "    assert isinstance(split_special_token, bytes), \"Must represent special token as a bytestring\"\n",
            "\n",
            "    # Get total file size in bytes\n",
            "    file.seek(0, os.SEEK_END)\n",
            "    file_size = file.tell()\n",
            "    file.seek(0)\n",
            "\n",
            "    chunk_size = file_size // desired_num_chunks\n",
            "\n",
            "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
            "    # Chunks start on previous index, don't include last index\n",
            "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
            "    chunk_boundaries[-1] = file_size\n",
            "\n",
            "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
            "\n",
            "    for bi in range(1, len(chunk_boundaries) - 1):\n",
            "        initial_position = chunk_boundaries[bi]\n",
            "        file.seek(initial_position)  # Start at boundary guess\n",
            "        while True:\n",
            "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
            "\n",
            "            # If EOF, this boundary should be at the end of the file\n",
            "            if mini_chunk == b\"\":\n",
            "                chunk_boundaries[bi] = file_size\n",
            "                break\n",
            "\n",
            "            # Find the special token in the mini chunk\n",
            "            found_at = mini_chunk.find(split_special_token)\n",
            "            if found_at != -1:\n",
            "                chunk_boundaries[bi] = initial_position + found_at\n",
            "                break\n",
            "            initial_position += mini_chunk_size\n",
            "\n",
            "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
            "    return sorted(set(chunk_boundaries))\n",
            "\n",
            "def update_token_freqs(\n",
            "    token_freqs: dict[TOKEN, int],\n",
            "    text_segment: str,\n",
            "):\n",
            "    matches = re.finditer(PAT, text_segment)\n",
            "    for m in matches:\n",
            "        token = m.group()\n",
            "        token_bytes = tuple(bytes([b]) for b in token.encode(\"utf-8\"))\n",
            "        token_freqs[token_bytes] += 1 \n",
            "\n",
            "def process_chunk(args):\n",
            "    input_path, start, end, special_tokens, chunk_id = args\n",
            "    # Read chunk\n",
            "    with open(input_path, 'rb') as f:\n",
            "        f.seek(start)\n",
            "        chunk_bytes = f.read(end - start)\n",
            "    \n",
            "    # Decode\n",
            "    chunk_str = chunk_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
            "    \n",
            "    # Logic from original train_bpe\n",
            "    special_pat = \"|\".join(re.escape(st) for st in special_tokens)\n",
            "    segments = re.split(special_pat, chunk_str)\n",
            "    \n",
            "    token_freqs = defaultdict(int)\n",
            "    # Use position=chunk_id+1 so 0 is left for the main bar\n",
            "    for seg in segments:\n",
            "        update_token_freqs(token_freqs, seg)\n",
            "    \n",
            "    return token_freqs\n",
            "\n",
            "def get_pairs(\n",
            "    token: TOKEN\n",
            ") -> list[PAIR]:\n",
            "    if len(token) < 2:\n",
            "        return []\n",
            "    return [(token[i], token[i+1]) for i in range(len(token)-1)]\n",
            "\n",
            "def update_pair_freqs(\n",
            "    pair_freqs: dict[PAIR, int],\n",
            "    token_freqs: dict[TOKEN, int],\n",
            "):\n",
            "    for token, freq in token_freqs.items():\n",
            "        if len(token) < 2:\n",
            "            continue\n",
            "        pairs = get_pairs(token)\n",
            "        for p in pairs:\n",
            "            pair_freqs[p] += freq\n",
            "\n",
            "def update_pair2idx(\n",
            "    pair_to_idx: dict[PAIR, set[TOKEN]],\n",
            "    token_freqs: dict[TOKEN, int],\n",
            "    token_to_idx: dict[TOKEN, int],\n",
            "):\n",
            "    for token, freq in token_freqs.items():\n",
            "        if len(token) < 2:\n",
            "            continue\n",
            "        pairs = get_pairs(token)\n",
            "        for p in pairs:\n",
            "            pair_to_idx[p].add(token_to_idx[token])\n",
            "\n",
            "\n",
            "def get_most_frequent_pair(\n",
            "    pair_freqs: dict[PAIR, int],\n",
            ") -> PAIR:\n",
            "    return max(pair_freqs.keys(), key=lambda k: (pair_freqs[k], k))\n",
            "\n",
            "def update_vocab(\n",
            "    vocab: dict[int, bytes],\n",
            "    new_id: int,\n",
            "    best_pair: PAIR,\n",
            "):\n",
            "    new_vocab = best_pair[0] + best_pair[1]\n",
            "    vocab[new_id] = new_vocab\n",
            "\n",
            "def update_vocab_inverse(\n",
            "    vocab_inverse: dict[bytes, int],\n",
            "    new_id: int,\n",
            "    best_pair: PAIR,\n",
            "):\n",
            "    new_vocab = best_pair[0] + best_pair[1]\n",
            "    vocab_inverse[new_vocab] = new_id\n",
            "\n",
            "def update_merges(\n",
            "    merges: list[PAIR],\n",
            "    best_pair: PAIR,\n",
            "):\n",
            "    merges.append(best_pair)\n",
            "\n",
            "def update_all(\n",
            "    pair_to_idx: dict[PAIR, set[int]],\n",
            "    pair_freqs: dict[PAIR, int],\n",
            "    token_freqs: dict[TOKEN, int],\n",
            "    idx_to_token: dict[int, TOKEN],\n",
            "    best_pair: PAIR,\n",
            "):\n",
            "    affected_idxs = list(pair_to_idx[best_pair])\n",
            "    merged_bytes = best_pair[0] + best_pair[1]\n",
            "\n",
            "    for idx in affected_idxs:\n",
            "        # get new token\n",
            "        i=0\n",
            "        token = idx_to_token[idx]\n",
            "        new_token = []\n",
            "        while(i<(len(token))):\n",
            "            if (i < len(token) - 1) and (token[i] == best_pair[0]) and (token[i+1] == best_pair[1]):\n",
            "                new_token.append(merged_bytes)\n",
            "                i = i + 2\n",
            "            else:\n",
            "                new_token.append(token[i])\n",
            "                i = i + 1\n",
            "        new_token = tuple(new_token)\n",
            "\n",
            "\n",
            "        ## update pair_to_idx\n",
            "        new_pairs = get_pairs(new_token)\n",
            "        affected_pairs = get_pairs(token)\n",
            "        for p in set(new_pairs)-set(affected_pairs):\n",
            "            pair_to_idx[p].add(idx)\n",
            "        for p in set(affected_pairs)-set(new_pairs):\n",
            "            pair_to_idx[p].discard(idx)\n",
            "\n",
            "\n",
            "        ## update pair_freqs\n",
            "        for pair in affected_pairs:\n",
            "            pair_freqs[pair] -= token_freqs[token]\n",
            "        for pair in new_pairs:\n",
            "            pair_freqs[pair] += token_freqs[token]\n",
            "\n",
            "\n",
            "        ## update token_freqs\n",
            "        origin_freq = token_freqs.pop(token)\n",
            "        token_freqs[new_token] = origin_freq\n",
            "\n",
            "        ## update idx_to_token\n",
            "        idx_to_token[idx] = new_token\n",
            "        \n",
            "\n",
            "def train_bpe(\n",
            "    input_path: str,\n",
            "    vocab_size: int,\n",
            "    special_tokens: list[str],\n",
            "    num_processes: int = 4\n",
            "):\n",
            "    print(\"Start Training BPE...\")\n",
            "    start_total = time.time()\n",
            "\n",
            "    print(\"Initialize Vocab and Merges...\")\n",
            "    vocab = {i:bytes([i]) for i in range(256)}\n",
            "    vocab_inverse = {v:k for k,v in vocab.items()}\n",
            "    # Add special tokens\n",
            "    for st in special_tokens:\n",
            "        new_id = len(vocab)\n",
            "        st_bytes = st.encode(\"utf-8\")\n",
            "        vocab[new_id] = st_bytes\n",
            "        vocab_inverse[st_bytes] = new_id\n",
            "    \n",
            "    merges = []\n",
            "\n",
            "    print(\"Calculating chunk boundaries...\")\n",
            "    t0 = time.time()\n",
            "    with open(input_path, 'rb') as f:\n",
            "        # Assuming first special token is the split token\n",
            "        split_token = special_tokens[0].encode(\"utf-8\") if special_tokens else b\"<|endoftext|>\"\n",
            "        boundaries = find_chunk_boundaries(f, num_processes, split_token) # More chunks than processes for load balancing\n",
            "    print(f\"Boundaries calculated in {time.time() - t0:.2f}s\")\n",
            "    \n",
            "    token_freqs = defaultdict(int)\n",
            "\n",
            "    ## Initiate token_freqs\n",
            "    print(\"Update Token Freq (Parallel)...\")\n",
            "    t0 = time.time()\n",
            "    \n",
            "    tasks = []\n",
            "    for i in range(len(boundaries) - 1):\n",
            "        start = boundaries[i]\n",
            "        end = boundaries[i+1]\n",
            "        tasks.append((input_path, start, end, special_tokens, i))\n",
            "    \n",
            "    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
            "        results = list(tqdm(executor.map(process_chunk, tasks), total=len(tasks), desc=\"Token Freqs (Chunks)\", position=0))\n",
            "        \n",
            "        for local_freqs in results:\n",
            "            for token, count in local_freqs.items():\n",
            "                token_freqs[token] += count\n",
            "                \n",
            "    idx_to_token = {i: k for i, k in enumerate(token_freqs.keys())}\n",
            "    token_to_idx = {k: i for i, k in enumerate(token_freqs.keys())}\n",
            "    print(f\"Update Token Freq took {time.time() - t0:.2f}s\")\n",
            "\n",
            "\n",
            "    ## Initiate pair_freqs, pair_to_token, and token_to_pair\n",
            "    print(\"Update Pair Freq...\")\n",
            "    t0 = time.time()\n",
            "    pair_freqs = defaultdict(int)\n",
            "    update_pair_freqs(pair_freqs, token_freqs)\n",
            "    print(f\"Update Pair Freq took {time.time() - t0:.2f}s\")\n",
            "\n",
            "    print(\"Update Pair to idx...\")\n",
            "    t0 = time.time()\n",
            "    pair_to_idx = defaultdict(set)\n",
            "    update_pair2idx(pair_to_idx, token_freqs, token_to_idx)\n",
            "    print(f\"Update Pair to Token took {time.time() - t0:.2f}s\")\n",
            "\n",
            "    print(\"Start Merging...\")\n",
            "    t0 = time.time()\n",
            "    num_merges = vocab_size - 256 - len(special_tokens)\n",
            "    for i in tqdm(range(num_merges), desc=\"Merging\"):\n",
            "        ### find most frequent pair\n",
            "        if not pair_freqs:\n",
            "            break\n",
            "        best_pair = get_most_frequent_pair(pair_freqs)\n",
            "\n",
            "        new_id = len(vocab)\n",
            "\n",
            "        ## update vocab\n",
            "        update_vocab(vocab, new_id, best_pair)\n",
            "\n",
            "        ## update vocab_inverse\n",
            "        update_vocab_inverse(vocab_inverse, new_id, best_pair)\n",
            "\n",
            "        ## update merges\n",
            "        update_merges(merges, best_pair)\n",
            "\n",
            "        update_all(pair_to_idx, pair_freqs, token_freqs, idx_to_token, best_pair)\n",
            "    print(f\"Merging took {time.time() - t0:.2f}s\")\n",
            "    print(f\"Total Training took {time.time() - start_total:.2f}s\")\n",
            "\n",
            "    return vocab, merges"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "b926ac39",
         "metadata": {},
         "source": [
            "#### Prepare"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "71344ec0",
         "metadata": {},
         "outputs": [],
         "source": [
            "test_string = \"abere ererea<|endoftext|>When and where is not as important as who and what. Hi, I am the the Ivan.<|endoftext|> aaa\"\n",
            "# test_string = \"abere ererea\"\n",
            "input_path = \"\"\n",
            "vocab_size = 260\n",
            "special_tokens = [END_TOKEN]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "id": "80e25f26",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'abere ererea'"
                  ]
               },
               "execution_count": 5,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "special_pat = \"|\".join(re.escape(st) for st in special_tokens)\n",
            "segments = re.split(special_pat, test_string)\n",
            "\n",
            "text_segment = segments[0]\n",
            "text_segment"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "3f0e1451",
         "metadata": {},
         "source": [
            "#### Initialize Vocab and Merges"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "id": "0cf7e249",
         "metadata": {},
         "outputs": [],
         "source": [
            "vocab = {i:bytes([i]) for i in range(256)}\n",
            "vocab_inverse = {v:k for k,v in vocab.items()}\n",
            "# Add special tokens\n",
            "for st in special_tokens:\n",
            "    new_id = len(vocab)\n",
            "    st_bytes = st.encode(\"utf-8\")\n",
            "    vocab[new_id] = st_bytes\n",
            "    vocab_inverse[st_bytes] = new_id\n",
            "\n",
            "merges = []"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "49aceb75",
         "metadata": {},
         "source": [
            "#### Update Token Freq"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "id": "ee804c47",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "defaultdict(int,\n",
                     "            {(b'a', b'b', b'e', b'r', b'e'): 1,\n",
                     "             (b' ', b'e', b'r', b'e', b'r', b'e', b'a'): 1})"
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "token_freqs = defaultdict(int)\n",
            "update_token_freqs(token_freqs,text_segment)\n",
            "token_freqs"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "c3561697",
         "metadata": {},
         "source": [
            "#### Update idx freq"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "id": "2c843b71",
         "metadata": {},
         "outputs": [],
         "source": [
            "idx_to_token = {i: k for i, k in enumerate(token_freqs.keys())}\n",
            "token_to_idx = {k: i for i, k in enumerate(token_freqs.keys())}\n",
            "idx_freqs = {token_to_idx[token]: freq for token, freq in token_freqs.items()}"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "0017256a",
         "metadata": {},
         "source": [
            "#### Update Pair Freq"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "id": "824040e0",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "defaultdict(int,\n",
                     "            {(b'a', b'b'): 1,\n",
                     "             (b'b', b'e'): 1,\n",
                     "             (b'e', b'r'): 3,\n",
                     "             (b'r', b'e'): 3,\n",
                     "             (b' ', b'e'): 1,\n",
                     "             (b'e', b'a'): 1})"
                  ]
               },
               "execution_count": 9,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "pair_freqs = defaultdict(int)\n",
            "update_pair_freqs(pair_freqs,token_freqs)\n",
            "pair_freqs"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "9a8ad346",
         "metadata": {},
         "source": [
            "#### Update Pair to idx"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "id": "892a9535",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "defaultdict(set,\n",
                     "            {(b'a', b'b'): {0},\n",
                     "             (b'b', b'e'): {0},\n",
                     "             (b'e', b'r'): {0, 1},\n",
                     "             (b'r', b'e'): {0, 1},\n",
                     "             (b' ', b'e'): {1},\n",
                     "             (b'e', b'a'): {1}})"
                  ]
               },
               "execution_count": 10,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "pair_to_idx = defaultdict(set)\n",
            "update_pair2idx(pair_to_idx, token_freqs, token_to_idx)\n",
            "pair_to_idx"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "fd9d7008",
         "metadata": {},
         "source": [
            "#### Merging"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "id": "99c45417",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "(b'r', b'e')"
                  ]
               },
               "execution_count": 11,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "best_pair = get_most_frequent_pair(pair_freqs)\n",
            "best_pair"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "id": "6d55fb3b",
         "metadata": {},
         "outputs": [],
         "source": [
            "new_id = len(vocab)\n",
            "\n",
            "## update vocab\n",
            "update_vocab(vocab, new_id, best_pair)\n",
            "\n",
            "## update vocab_inverse\n",
            "update_vocab_inverse(vocab_inverse, new_id, best_pair)\n",
            "\n",
            "## update merges\n",
            "update_merges(merges, best_pair)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "id": "b9a1fc7e",
         "metadata": {},
         "outputs": [],
         "source": [
            "affected_idxs = list(pair_to_idx[best_pair])\n",
            "merged_bytes = best_pair[0] + best_pair[1]\n",
            "for idx in affected_idxs:\n",
            "    token = idx_to_token[idx]\n",
            "\n",
            "    i=0\n",
            "    new_token = []\n",
            "    while(i<(len(token))):\n",
            "        if (i < len(token) - 1) and (token[i] == best_pair[0]) and (token[i+1] == best_pair[1]):\n",
            "            new_token.append(merged_bytes)\n",
            "            i = i + 2\n",
            "        else:\n",
            "            new_token.append(token[i])\n",
            "            i = i + 1\n",
            "    new_token = tuple(new_token)\n",
            "\n",
            "    new_pairs = get_pairs(new_token)\n",
            "    affected_pairs = get_pairs(token)\n",
            "    for p in set(new_pairs)-set(affected_pairs):\n",
            "        pair_to_idx[p].add(idx)\n",
            "    for p in set(affected_pairs)-set(new_pairs):\n",
            "        pair_to_idx[p].discard(idx)\n",
            "\n",
            "\n",
            "    for pair in affected_pairs:\n",
            "        pair_freqs[pair] -= idx_freqs[idx]\n",
            "    for pair in new_pairs:\n",
            "        pair_freqs[pair] += idx_freqs[idx]\n",
            "\n",
            "    # origin_freq = token_freqs.pop(token)\n",
            "    # token_freqs[new_token] = origin_freq\n",
            "\n",
            "    idx_to_token[idx] = new_token"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "id": "6622a581",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "defaultdict(set,\n",
                     "            {(b'a', b'b'): {0},\n",
                     "             (b'b', b'e'): {0},\n",
                     "             (b'e', b'r'): set(),\n",
                     "             (b'r', b'e'): set(),\n",
                     "             (b' ', b'e'): {1},\n",
                     "             (b'e', b'a'): set(),\n",
                     "             (b'e', b're'): {0, 1},\n",
                     "             (b're', b'a'): {1},\n",
                     "             (b're', b're'): {1}})"
                  ]
               },
               "execution_count": 18,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "pair_to_idx"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "id": "1c8bc38a",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "defaultdict(int,\n",
                     "            {(b'a', b'b'): 1,\n",
                     "             (b'b', b'e'): 1,\n",
                     "             (b'e', b'r'): 0,\n",
                     "             (b'r', b'e'): 0,\n",
                     "             (b' ', b'e'): 1,\n",
                     "             (b'e', b'a'): 0,\n",
                     "             (b'e', b're'): 2,\n",
                     "             (b're', b're'): 1,\n",
                     "             (b're', b'a'): 1})"
                  ]
               },
               "execution_count": 19,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "pair_freqs"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 86,
         "id": "a4f47552",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{0: 1, 1: 1}"
                  ]
               },
               "execution_count": 86,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "idx_freqs"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 46,
         "id": "bcb800c0",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{0: (b'a', b'b', b'e', b're'), 1: (b' ', b'e', b're', b're', b'a')}"
                  ]
               },
               "execution_count": 46,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "idx_to_token"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "ca3e2ea9",
         "metadata": {},
         "source": [
            "### 2.6 BPE Tokenizer: Encoding and Decoding"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "1c3506f3",
         "metadata": {},
         "source": [
            "##### 2.6.1 Encoding text"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "8ac9f7ff",
         "metadata": {},
         "source": [
            "##### 2.6.2 Decoding text"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "c2a85b24",
         "metadata": {},
         "outputs": [],
         "source": [
            "class Tokenizer():\n",
            "    def __init__(\n",
            "        self, \n",
            "        vocab: dict[int, bytes], \n",
            "        merges: list[PAIR],\n",
            "        special_tokens: list[str] | None = None,\n",
            "    ):\n",
            "        self.vocab = vocab\n",
            "        self.merges = merges\n",
            "        \n",
            "        if special_tokens:\n",
            "            new_index = len(vocab)\n",
            "            for st in special_tokens:\n",
            "                if st != END_TOKEN:\n",
            "                    vocab[new_index] = st\n",
            "                    new_index += 1\n",
            "            self.special_tokens =  sorted(list(set([END_TOKEN]+special_tokens)), key=len, reverse=True)\n",
            "        else:\n",
            "            self.special_tokens = [END_TOKEN]\n",
            "\n",
            "        self.vocab_inverse = {v: k for k, v in vocab.items()}\n",
            "        self.merges_to_rank = {m: i for i, m in enumerate(merges)}\n",
            "\n",
            "    @classmethod\n",
            "    def from_files(\n",
            "        cls, \n",
            "        vocab_filepath: str, \n",
            "        merges_filepath: str, \n",
            "        special_tokens: list[str] | None = None,\n",
            "    ) :\n",
            "        with open(vocab_filepath, \"rb\") as f:\n",
            "            vocab = pickle.load(f)\n",
            "        with open(merges_filepath, \"rb\") as f:\n",
            "            merges = pickle.load(f)\n",
            "        return cls(vocab, merges, special_tokens=special_tokens)\n",
            "\n",
            "    def merge_tokens(self, token_bytes: list[bytes]) -> list[bytes]:\n",
            "        if len(token_bytes) <= 1:\n",
            "            return token_bytes\n",
            "        if token_bytes in self.special_tokens:\n",
            "            return token_bytes\n",
            "        while 1:\n",
            "            merge_position = -1\n",
            "            smallest_rank = len(self.merges)\n",
            "            for i in range(len(token_bytes)-1):\n",
            "                current_pair = (token_bytes[i], token_bytes[i+1])\n",
            "                rank = self.merges_to_rank.get(current_pair, -1)\n",
            "                if rank == -1:\n",
            "                    continue\n",
            "                if rank < smallest_rank:\n",
            "                    smallest_rank = rank\n",
            "                    merge_position = i\n",
            "            if merge_position == -1:\n",
            "                break\n",
            "            token_bytes = token_bytes[:(merge_position)] + [token_bytes[merge_position]+token_bytes[merge_position+1]]+token_bytes[(merge_position+2):]\n",
            "        return token_bytes\n",
            "\n",
            "    def encode(self, text: str) -> list[int]:\n",
            "        special_pat = \"(\"+\"|\".join(re.escape(st) for st in self.special_tokens)+\")\"\n",
            "        segments = re.split(special_pat, text)\n",
            "        ids = []\n",
            "        for segment in segments:\n",
            "            if not segment:\n",
            "                continue\n",
            "            if segment in self.special_tokens:\n",
            "                token_bytes = self.transfer_text2bytes(segment)\n",
            "                token_bytes = self.merge_tokens(token_bytes)\n",
            "                encoded_token = []\n",
            "                for i in token_bytes:\n",
            "                    encoded_token.append(self.vocab_inverse[i])\n",
            "                ids.extend(encoded_token)\n",
            "                continue\n",
            "            matches = re.finditer(PAT, segment)\n",
            "            for m in matches:\n",
            "                token = m.group()\n",
            "                token_bytes = self.transfer_text2bytes(token)\n",
            "                token_bytes = self.merge_tokens(token_bytes)\n",
            "                encoded_token = []\n",
            "                for i in token_bytes:\n",
            "                    encoded_token.append(self.vocab_inverse[i])\n",
            "                ids.extend(encoded_token)\n",
            "        return ids\n",
            "\n",
            "    def _encode_batch(self, texts: list[str]) -> list[list[int]]:\n",
            "        return [self.encode(text) for text in texts]\n",
            "\n",
            "    def encode_iterable(self, iterable: Iterable[str], num_processes: int = 4, batch_size: int = 1000) -> Iterator[int]:\n",
            "        with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
            "            def batch_generator():\n",
            "                current_batch = []\n",
            "                for text in iterable:\n",
            "                    current_batch.append(text)\n",
            "                    if len(current_batch) >= batch_size:\n",
            "                        yield current_batch\n",
            "                        current_batch = []\n",
            "                if current_batch:\n",
            "                    yield current_batch\n",
            "            # 直接在 encode_iterable 这一层产出结果\n",
            "            for batch_results in executor.map(self._encode_batch, batch_generator()):\n",
            "                for seq in batch_results:\n",
            "                    yield from seq\n",
            "\n",
            "    def decode(self, ids: list[int]) -> str:\n",
            "        unk_bytes = '\\ufffd'.encode('utf-8')\n",
            "        bytes_list = [self.vocab.get(i, unk_bytes) for i in ids]\n",
            "        return b\"\".join(bytes_list).decode(\"utf-8\", errors=\"replace\")\n",
            "\n",
            "    def transfer_text2bytes(self, segment: str) -> list[bytes]:\n",
            "        if segment in self.special_tokens:\n",
            "            return [segment.encode(\"utf-8\")]\n",
            "        token_bytes = list(bytes([b]) for b in segment.encode(\"utf-8\"))\n",
            "        return token_bytes"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "id": "ff49ad7c",
         "metadata": {},
         "outputs": [],
         "source": [
            "import tiktoken\n",
            "from tests.adapters import get_tokenizer\n",
            "from tests.common import FIXTURES_PATH, gpt2_bytes_to_unicode\n",
            "\n",
            "VOCAB_PATH = FIXTURES_PATH / \"gpt2_vocab.json\"\n",
            "MERGES_PATH = FIXTURES_PATH / \"gpt2_merges.txt\"\n",
            "\n",
            "from tests.test_tokenizer import get_tokenizer_from_vocab_merges_path"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "25bd8bdf",
         "metadata": {},
         "source": [
            "### 2.7 Experiments"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "id": "ac03fcb4",
         "metadata": {},
         "outputs": [],
         "source": [
            "from cs336_basics.bpe_tokenize import Tokenizer,find_chunk_boundaries,END_TOKEN"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "8e4ba93e",
         "metadata": {},
         "outputs": [],
         "source": [
            "input_path = f\"{project_root}/data/TinyStoriesV2-GPT4-valid.txt\"\n",
            "special_tokens = [END_TOKEN]\n",
            "\n",
            "data_group = \"train\"\n",
            "vocab_size = 10000\n",
            "file_name = f\"TinyStoriesV2-GPT4-{data_group}.txt\"\n",
            "vocab_filepath = f\"{project_root}/outputs/{file_name.split(\".\")[0]}-vocab-{vocab_size}.pkl\"\n",
            "merges_filepath = f\"{project_root}/outputs/{file_name.split(\".\")[0]}-merge-{vocab_size}.pkl\"\n",
            "\n",
            "\n",
            "num_processes = 16\n",
            "\n",
            "\n",
            "tokenizer = Tokenizer.from_files(vocab_filepath,merges_filepath,special_tokens)\n",
            "\n",
            "with open(input_path, 'rb') as f:\n",
            "    split_token = special_tokens[0].encode(\"utf-8\") if special_tokens else b\"\"\n",
            "    boundaries = find_chunk_boundaries(f, num_processes, split_token) \n",
            "\n",
            "start = boundaries[0]\n",
            "end = boundaries[1]\n",
            "\n",
            "with open(input_path, 'rb') as f:\n",
            "    f.seek(start)\n",
            "    chunk_bytes = f.read(end - start)\n",
            "chunk_str = chunk_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
            "special_pat = \"|\".join(re.escape(st) for st in special_tokens)\n",
            "segments = re.split(special_pat, chunk_str)\n",
            "\n",
            "ran_ints = random.sample(range(len(segments)), 10)\n",
            "for i in ran_ints:\n",
            "    ids = tokenizer.encode(segments[i])\n",
            "    print(len(ids)/len(segments[ran_int]))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 43,
         "id": "a09b9de2",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "0.2624356775300172\n",
                  "0.2933104631217839\n",
                  "0.6792452830188679\n",
                  "0.3567753001715266\n",
                  "0.18181818181818182\n",
                  "0.32246998284734135\n",
                  "0.2692967409948542\n",
                  "0.274442538593482\n",
                  "0.3687821612349914\n",
                  "0.2109777015437393\n"
               ]
            }
         ],
         "source": [
            "special_tokens = [END_TOKEN]\n",
            "\n",
            "data_group = \"train\"\n",
            "vocab_size = 10000\n",
            "file_name = f\"TinyStoriesV2-GPT4-{data_group}.txt\"\n",
            "input_path = f\"{project_root}/data/{file_name}\"\n",
            "vocab_filepath = f\"{project_root}/outputs/{file_name.split(\".\")[0]}-vocab-{vocab_size}.pkl\"\n",
            "merges_filepath = f\"{project_root}/outputs/{file_name.split(\".\")[0]}-merge-{vocab_size}.pkl\"\n",
            "\n",
            "\n",
            "num_processes = 16\n",
            "\n",
            "\n",
            "tokenizer = Tokenizer.from_files(vocab_filepath,merges_filepath,special_tokens)\n",
            "\n",
            "with open(input_path, 'rb') as f:\n",
            "    split_token = special_tokens[0].encode(\"utf-8\") if special_tokens else b\"\"\n",
            "    boundaries = find_chunk_boundaries(f, num_processes, split_token) \n",
            "\n",
            "start = boundaries[0]\n",
            "end = boundaries[1]\n",
            "\n",
            "with open(input_path, 'rb') as f:\n",
            "    f.seek(start)\n",
            "    chunk_bytes = f.read(end - start)\n",
            "chunk_str = chunk_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
            "special_pat = \"|\".join(re.escape(st) for st in special_tokens)\n",
            "segments = re.split(special_pat, chunk_str)\n",
            "\n",
            "ran_ints = random.sample(range(len(segments)), 10)\n",
            "for i in ran_ints:\n",
            "    ids = tokenizer.encode(segments[i])\n",
            "    print(len(ids)/len(segments[ran_int]))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 45,
         "id": "554bd4aa",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "1.5901759530791788\n",
                  "0.2631964809384164\n",
                  "0.748533724340176\n",
                  "1.0425219941348973\n",
                  "0.9919354838709677\n",
                  "0.6708211143695014\n",
                  "0.2749266862170088\n",
                  "1.5879765395894427\n",
                  "0.41642228739002934\n",
                  "0.39222873900293254\n"
               ]
            }
         ],
         "source": [
            "special_tokens = [END_TOKEN]\n",
            "\n",
            "data_group = \"train\"\n",
            "vocab_size = 32000\n",
            "file_name = f\"owt_{data_group}.txt\"\n",
            "input_path = f\"{project_root}/data/{file_name}\"\n",
            "vocab_filepath = f\"{project_root}/outputs/{file_name.split(\".\")[0]}-vocab-{vocab_size}.pkl\"\n",
            "merges_filepath = f\"{project_root}/outputs/{file_name.split(\".\")[0]}-merge-{vocab_size}.pkl\"\n",
            "\n",
            "\n",
            "num_processes = 16\n",
            "\n",
            "\n",
            "tokenizer = Tokenizer.from_files(vocab_filepath,merges_filepath,special_tokens)\n",
            "\n",
            "with open(input_path, 'rb') as f:\n",
            "    split_token = special_tokens[0].encode(\"utf-8\") if special_tokens else b\"\"\n",
            "    boundaries = find_chunk_boundaries(f, num_processes, split_token) \n",
            "\n",
            "start = boundaries[0]\n",
            "end = boundaries[1]\n",
            "\n",
            "with open(input_path, 'rb') as f:\n",
            "    f.seek(start)\n",
            "    chunk_bytes = f.read(end - start)\n",
            "chunk_str = chunk_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
            "special_pat = \"|\".join(re.escape(st) for st in special_tokens)\n",
            "segments = re.split(special_pat, chunk_str)\n",
            "\n",
            "ran_ints = random.sample(range(len(segments)), 10)\n",
            "for i in ran_ints:\n",
            "    ids = tokenizer.encode(segments[i])\n",
            "    print(len(ids)/len(segments[ran_int]))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 46,
         "id": "c029c1c3",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "0.2641509433962264\n",
                  "0.27101200686106347\n",
                  "0.3584905660377358\n",
                  "0.3516295025728988\n",
                  "0.3584905660377358\n",
                  "0.3156089193825043\n",
                  "0.8576329331046312\n",
                  "0.5728987993138936\n",
                  "0.27958833619210977\n",
                  "0.4922813036020583\n"
               ]
            }
         ],
         "source": [
            "special_tokens = [END_TOKEN]\n",
            "\n",
            "data_group = \"train\"\n",
            "vocab_size = 32000\n",
            "file_name = f\"owt_{data_group}.txt\"\n",
            "input_path = f\"{project_root}/data/TinyStoriesV2-GPT4-train.txt\"\n",
            "vocab_filepath = f\"{project_root}/outputs/{file_name.split(\".\")[0]}-vocab-{vocab_size}.pkl\"\n",
            "merges_filepath = f\"{project_root}/outputs/{file_name.split(\".\")[0]}-merge-{vocab_size}.pkl\"\n",
            "\n",
            "\n",
            "num_processes = 16\n",
            "\n",
            "\n",
            "tokenizer = Tokenizer.from_files(vocab_filepath,merges_filepath,special_tokens)\n",
            "\n",
            "with open(input_path, 'rb') as f:\n",
            "    split_token = special_tokens[0].encode(\"utf-8\") if special_tokens else b\"\"\n",
            "    boundaries = find_chunk_boundaries(f, num_processes, split_token) \n",
            "\n",
            "start = boundaries[0]\n",
            "end = boundaries[1]\n",
            "\n",
            "with open(input_path, 'rb') as f:\n",
            "    f.seek(start)\n",
            "    chunk_bytes = f.read(end - start)\n",
            "chunk_str = chunk_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
            "special_pat = \"|\".join(re.escape(st) for st in special_tokens)\n",
            "segments = re.split(special_pat, chunk_str)\n",
            "\n",
            "ran_ints = random.sample(range(len(segments)), 10)\n",
            "for i in ran_ints:\n",
            "    ids = tokenizer.encode(segments[i])\n",
            "    print(len(ids)/len(segments[ran_int]))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 31,
         "id": "b60a2e9f",
         "metadata": {},
         "outputs": [],
         "source": [
            "ran_int = ran_ints[0]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 34,
         "id": "a69aa2f4",
         "metadata": {},
         "outputs": [],
         "source": [
            "ids = tokenizer.encode(segments[ran_int])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "10bc4ae2",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "0.24390243902439024"
                  ]
               },
               "execution_count": 35,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": []
      },
      {
         "cell_type": "markdown",
         "id": "9e4bcb86",
         "metadata": {},
         "source": [
            "## 3 Transformer Language Model Architecture"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "968bd4f9",
         "metadata": {},
         "source": [
            "### 3.1 Transformer LM"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "582898d8",
         "metadata": {},
         "source": [
            "#### 3.1.1 Token Embeddings"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "ad60c47a",
         "metadata": {},
         "source": [
            "#### 3.1.2 Pre-norm Transformer Block"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "2929ee06",
         "metadata": {},
         "source": [
            "### 3.2 Output Normalization and Embedding"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "9510b583",
         "metadata": {},
         "source": [
            "### 3.3 Remark: Batching, Einsum and Eﬀicient Computation"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "id": "40645c38",
         "metadata": {},
         "outputs": [],
         "source": [
            "images = torch.randn(64, 128, 128, 3) # (batch, height, width, channel)\n",
            "dim_by = torch.linspace(start=0.0, end=1.0, steps=10)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "id": "530a17d5",
         "metadata": {},
         "outputs": [],
         "source": [
            "dim_value = rearrange(dim_by, \"dim_value -> 1 dim_value 1 1 1\")\n",
            "images_rearr = rearrange(images, \"b height width channel -> b 1 height width channel\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "id": "67743218",
         "metadata": {},
         "outputs": [],
         "source": [
            "dimmed_images = images_rearr * dim_value"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "id": "3438c810",
         "metadata": {},
         "outputs": [],
         "source": [
            "dimmed_images = einsum(\n",
            "    images, dim_by,\n",
            "    \"batch height width channel, dim_value -> batch dim_value height width channel\"\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "id": "688b2df1",
         "metadata": {},
         "outputs": [],
         "source": [
            "channels_last = torch.randn(64, 32, 32, 3) # (batch, height, width, channel)\n",
            "B = torch.randn(32*32, 32*32)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "f8e1bfb1",
         "metadata": {},
         "outputs": [],
         "source": [
            "## Rearrange an image tensor for mixing across all pixels\n",
            "channels_last_flat = channels_last.view(\n",
            "    -1, channels_last.size(1) * channels_last.size(2), channels_last.size(3)\n",
            ")\n",
            "channels_first_flat = channels_last_flat.transpose(1, 2)\n",
            "channels_first_flat_transformed = channels_first_flat @ B.T\n",
            "channels_last_flat_transformed = channels_first_flat_transformed.transpose(1, 2)\n",
            "channels_last_transformed = channels_last_flat_transformed.view(*channels_last.shape)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "id": "cdf7de6f",
         "metadata": {},
         "outputs": [],
         "source": [
            "height = width = 32\n",
            "## Rearrange replaces clunky torch view + transpose\n",
            "channels_first = rearrange(\n",
            "    channels_last,\n",
            "    \"batch height width channel -> batch channel (height width)\"\n",
            ")\n",
            "channels_first_transformed = einsum(\n",
            "    channels_first, B,\n",
            "    \"batch channel pixel_in, pixel_out pixel_in -> batch channel pixel_out\"\n",
            ")\n",
            "channels_last_transformed = rearrange(\n",
            "    channels_first_transformed,\n",
            "    \"batch channel (height width) -> batch height width channel\",\n",
            "    height=height, width=width\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 21,
         "id": "4b25c164",
         "metadata": {},
         "outputs": [
            {
               "ename": "NameError",
               "evalue": "name 'einx' is not defined",
               "output_type": "error",
               "traceback": [
                  "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                  "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                  "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m height = width = \u001b[32m32\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m channels_last_transformed = \u001b[43meinx\u001b[49m.dot(\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbatch row_in col_in channel, (row_out col_out) (row_in col_in)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m-> batch row_out col_out channel\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     channels_last, B,\n\u001b[32m      6\u001b[39m     col_in=width, col_out=width\n\u001b[32m      7\u001b[39m )\n",
                  "\u001b[31mNameError\u001b[39m: name 'einx' is not defined"
               ]
            }
         ],
         "source": [
            "height = width = 32\n",
            "channels_last_transformed = einx.dot(\n",
            "    \"batch row_in col_in channel, (row_out col_out) (row_in col_in)\"\n",
            "    \"-> batch row_out col_out channel\",\n",
            "    channels_last, B,\n",
            "    col_in=width, col_out=width\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "c4581dc1",
         "metadata": {},
         "source": [
            "#### 3.3.1 Mathematical Notation and Memory Ordering"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "1263a6a4",
         "metadata": {},
         "source": [
            "### 3.4 Basic Building Blocks: Linear and Embedding Modules"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "33dcbaf1",
         "metadata": {},
         "source": [
            "#### 3.4.1 Parameter Initialization"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "fded2dc4",
         "metadata": {},
         "source": [
            "#### 3.4.2 Linear Module"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "3527dc9a",
         "metadata": {},
         "outputs": [],
         "source": [
            "class Linear(nn.Module):\n",
            "    def __init__(\n",
            "        self, \n",
            "        in_features: int, \n",
            "        out_features: int, \n",
            "        device: torch.device | None = None, \n",
            "        dtype: torch.dtype | None = None\n",
            "    ):\n",
            "        super().__init__()\n",
            "        self.in_features = in_features\n",
            "        self.out_features = out_features\n",
            "        self.device = device\n",
            "        self.dtype = dtype\n",
            "\n",
            "        self.weights = nn.Parameter(\n",
            "            torch.empty(out_features, in_features, device=self.device, dtype=self.dtype)\n",
            "        )\n",
            "        std = np.sqrt(2/(in_features+out_features))\n",
            "        nn.init.trunc_normal_(\n",
            "            self.weights,\n",
            "            mean = 0,\n",
            "            std=std,\n",
            "            a=-3*std,\n",
            "            b=3*std\n",
            "        )\n",
            "\n",
            "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
            "        '''\n",
            "        Apply the linear transformation to the input\n",
            "        '''\n",
            "        output = insum(\n",
            "            x, self.weights,\n",
            "            \"... in_features, out_features in_features -> ... out_features\"\n",
            "        )\n",
            "        return output\n",
            "        "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 26,
         "id": "e8a8e752",
         "metadata": {},
         "outputs": [],
         "source": [
            "in_features = 4\n",
            "out_features = 2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 31,
         "id": "254160f9",
         "metadata": {},
         "outputs": [],
         "source": [
            "weights = nn.Parameter(\n",
            "            torch.empty(out_features, in_features)\n",
            "        )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "e881816a",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "Parameter containing:\n",
                     "tensor([[-0.3163,  0.2574, -0.6433, -1.2062],\n",
                     "        [-0.2289,  0.3611,  0.4565, -0.2161]], requires_grad=True)"
                  ]
               },
               "execution_count": 35,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "std = np.sqrt(2/(in_features+out_features))\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "d5dc7d33",
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.12"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
