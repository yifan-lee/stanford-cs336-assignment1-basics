{
   "cells": [
      {
         "cell_type": "markdown",
         "id": "153fa639",
         "metadata": {},
         "source": [
            "## Start"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "id": "2da7993a",
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "import pickle\n",
            "import time\n",
            "\n",
            "import regex as re\n",
            "import numpy as np\n",
            "\n",
            "from tqdm import tqdm\n",
            "from concurrent.futures import ProcessPoolExecutor\n",
            "from typing import BinaryIO\n",
            "from collections import defaultdict\n",
            "from collections.abc import Iterable, Iterator"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "id": "f8f7a596",
         "metadata": {},
         "outputs": [],
         "source": [
            "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
            "TOKEN = tuple[bytes]\n",
            "PAIR = tuple[bytes, bytes]\n",
            "END_TOKEN = '<|endoftext|>'"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "a2f2e7f3",
         "metadata": {},
         "source": [
            "## 1 Assignment Overview"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "f54e287b",
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "markdown",
         "id": "2a80feac",
         "metadata": {},
         "source": [
            "## 2 Byte-Pair Encoding (BPE) Tokenizer"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "6fcdbb3e",
         "metadata": {},
         "source": [
            "### 2.1 The Unicode Standard"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "id": "c9bf5df8",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[29275, '牛']"
                  ]
               },
               "execution_count": 3,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "[ord('牛'), chr(29275)]"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "a209ddd0",
         "metadata": {},
         "source": [
            "#### Problem (unicode1)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "bb1d9697",
         "metadata": {},
         "source": [
            "##### a"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "45b90bd1",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'\\x00'"
                  ]
               },
               "execution_count": 3,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "chr(0)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "46f463d9",
         "metadata": {},
         "source": [
            "##### b"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "c91f9fdb",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\u0000\n"
               ]
            }
         ],
         "source": [
            "print(chr(0))"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "01b6078b",
         "metadata": {},
         "source": [
            "##### c"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "id": "96daa256",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'this is a test\\x00string'"
                  ]
               },
               "execution_count": 8,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "\"this is a test\" + chr(0) + \"string\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "id": "86647719",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "this is a test\u0000string\n"
               ]
            }
         ],
         "source": [
            "print(\"this is a test\" + chr(0) + \"string\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "885fe342",
         "metadata": {},
         "source": [
            "### 2.2 Unicode Encodings"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "id": "12077a57",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n"
               ]
            }
         ],
         "source": [
            "test_string = \"hello! こんにちは!\"\n",
            "utf8_encoded = test_string.encode(\"utf-8\")\n",
            "print(utf8_encoded)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "id": "dc24edd9",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "<class 'bytes'>\n"
               ]
            }
         ],
         "source": [
            "print(type(utf8_encoded))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "id": "7c38939e",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[104,\n",
                     " 101,\n",
                     " 108,\n",
                     " 108,\n",
                     " 111,\n",
                     " 33,\n",
                     " 32,\n",
                     " 227,\n",
                     " 129,\n",
                     " 147,\n",
                     " 227,\n",
                     " 130,\n",
                     " 147,\n",
                     " 227,\n",
                     " 129,\n",
                     " 171,\n",
                     " 227,\n",
                     " 129,\n",
                     " 161,\n",
                     " 227,\n",
                     " 129,\n",
                     " 175,\n",
                     " 33]"
                  ]
               },
               "execution_count": 12,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "list(utf8_encoded)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "id": "3322f2e7",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[13, 23]"
                  ]
               },
               "execution_count": 13,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "[len(test_string),len(utf8_encoded)]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "id": "b6bb20ca",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "hello! こんにちは!\n"
               ]
            }
         ],
         "source": [
            "print(utf8_encoded.decode(\"utf-8\"))"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "0b18de75",
         "metadata": {},
         "source": [
            "#### Problem (unicode2)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "ed709893",
         "metadata": {},
         "source": [
            "##### a"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "faf37160",
         "metadata": {},
         "source": [
            "##### b"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "id": "f8cfb916",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'hello'"
                  ]
               },
               "execution_count": 15,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
            "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
            "decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "id": "35a3b6be",
         "metadata": {},
         "outputs": [
            {
               "ename": "UnicodeDecodeError",
               "evalue": "'utf-8' codec can't decode byte 0xe4 in position 0: unexpected end of data",
               "output_type": "error",
               "traceback": [
                  "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                  "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
                  "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhello, 你好\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
                  "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mdecode_utf8_bytes_to_str_wrong\u001b[39m\u001b[34m(bytestring)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
                  "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xe4 in position 0: unexpected end of data"
               ]
            }
         ],
         "source": [
            "decode_utf8_bytes_to_str_wrong(\"hello, 你好\".encode(\"utf-8\"))"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "8b68518c",
         "metadata": {},
         "source": [
            "##### c"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "id": "5b92654d",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[b'h',\n",
                     " b'e',\n",
                     " b'l',\n",
                     " b'l',\n",
                     " b'o',\n",
                     " b',',\n",
                     " b' ',\n",
                     " b'\\xe4',\n",
                     " b'\\xbd',\n",
                     " b'\\xa0',\n",
                     " b'\\xe5',\n",
                     " b'\\xa5',\n",
                     " b'\\xbd']"
                  ]
               },
               "execution_count": 18,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "sentence = \"hello, 你好\".encode(\"utf-8\")\n",
            "[bytes([b]) for b in sentence]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 24,
         "id": "2713459a",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'你'"
                  ]
               },
               "execution_count": 24,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "(b'\\xe4\\xbd\\xa0').decode(\"utf-8\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "259e6159",
         "metadata": {},
         "source": [
            "### 2.3 Subword Tokenization"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "5eb7babd",
         "metadata": {},
         "source": [
            "### 2.4 BPE Tokenizer Training"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 27,
         "id": "1290735e",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
                  ]
               },
               "execution_count": 27,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "re.findall(PAT, \"some text that i'll pre-tokenize\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 28,
         "id": "1de4c71d",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "('BA', 'A')"
                  ]
               },
               "execution_count": 28,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "max([(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 31,
         "id": "1afabb34",
         "metadata": {},
         "outputs": [],
         "source": [
            "test_string = \"\"\"low low low low low lower lower widest widest widest newest newest newest newest newest newest\"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 32,
         "id": "6f73644c",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "['low',\n",
                     " ' low',\n",
                     " ' low',\n",
                     " ' low',\n",
                     " ' low',\n",
                     " ' lower',\n",
                     " ' lower',\n",
                     " ' widest',\n",
                     " ' widest',\n",
                     " ' widest',\n",
                     " ' newest',\n",
                     " ' newest',\n",
                     " ' newest',\n",
                     " ' newest',\n",
                     " ' newest',\n",
                     " ' newest']"
                  ]
               },
               "execution_count": 32,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "re.findall(PAT, test_string)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "30f545de",
         "metadata": {},
         "source": [
            "### 2.5 Experimenting with BPE Tokenizer Training"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "id": "5143d682",
         "metadata": {},
         "outputs": [],
         "source": [
            "def find_chunk_boundaries(\n",
            "    file: BinaryIO,\n",
            "    desired_num_chunks: int,\n",
            "    split_special_token: bytes,\n",
            ") -> list[int]:\n",
            "    \"\"\"\n",
            "    Chunk the file into parts that can be counted independently.\n",
            "    May return fewer chunks if the boundaries end up overlapping.\n",
            "    \"\"\"\n",
            "    assert isinstance(split_special_token, bytes), \"Must represent special token as a bytestring\"\n",
            "\n",
            "    # Get total file size in bytes\n",
            "    file.seek(0, os.SEEK_END)\n",
            "    file_size = file.tell()\n",
            "    file.seek(0)\n",
            "\n",
            "    chunk_size = file_size // desired_num_chunks\n",
            "\n",
            "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
            "    # Chunks start on previous index, don't include last index\n",
            "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
            "    chunk_boundaries[-1] = file_size\n",
            "\n",
            "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
            "\n",
            "    for bi in range(1, len(chunk_boundaries) - 1):\n",
            "        initial_position = chunk_boundaries[bi]\n",
            "        file.seek(initial_position)  # Start at boundary guess\n",
            "        while True:\n",
            "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
            "\n",
            "            # If EOF, this boundary should be at the end of the file\n",
            "            if mini_chunk == b\"\":\n",
            "                chunk_boundaries[bi] = file_size\n",
            "                break\n",
            "\n",
            "            # Find the special token in the mini chunk\n",
            "            found_at = mini_chunk.find(split_special_token)\n",
            "            if found_at != -1:\n",
            "                chunk_boundaries[bi] = initial_position + found_at\n",
            "                break\n",
            "            initial_position += mini_chunk_size\n",
            "\n",
            "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
            "    return sorted(set(chunk_boundaries))\n",
            "\n",
            "def update_token_freqs(\n",
            "    token_freqs: dict[TOKEN, int],\n",
            "    text_segment: str,\n",
            "):\n",
            "    matches = re.finditer(PAT, text_segment)\n",
            "    for m in matches:\n",
            "        token = m.group()\n",
            "        token_bytes = tuple(bytes([b]) for b in token.encode(\"utf-8\"))\n",
            "        token_freqs[token_bytes] += 1 \n",
            "\n",
            "def process_chunk(args):\n",
            "    input_path, start, end, special_tokens, chunk_id = args\n",
            "    # Read chunk\n",
            "    with open(input_path, 'rb') as f:\n",
            "        f.seek(start)\n",
            "        chunk_bytes = f.read(end - start)\n",
            "    \n",
            "    # Decode\n",
            "    chunk_str = chunk_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
            "    \n",
            "    # Logic from original train_bpe\n",
            "    special_pat = \"|\".join(re.escape(st) for st in special_tokens)\n",
            "    segments = re.split(special_pat, chunk_str)\n",
            "    \n",
            "    token_freqs = defaultdict(int)\n",
            "    # Use position=chunk_id+1 so 0 is left for the main bar\n",
            "    for seg in segments:\n",
            "        update_token_freqs(token_freqs, seg)\n",
            "    \n",
            "    return token_freqs\n",
            "\n",
            "def get_pairs(\n",
            "    token: TOKEN\n",
            ") -> list[PAIR]:\n",
            "    if len(token) < 2:\n",
            "        return []\n",
            "    return [(token[i], token[i+1]) for i in range(len(token)-1)]\n",
            "\n",
            "def update_pair_freqs(\n",
            "    pair_freqs: dict[PAIR, int],\n",
            "    token_freqs: dict[TOKEN, int],\n",
            "):\n",
            "    for token, freq in token_freqs.items():\n",
            "        if len(token) < 2:\n",
            "            continue\n",
            "        pairs = get_pairs(token)\n",
            "        for p in pairs:\n",
            "            pair_freqs[p] += freq\n",
            "\n",
            "def update_pair2idx(\n",
            "    pair_to_idx: dict[PAIR, set[TOKEN]],\n",
            "    token_freqs: dict[TOKEN, int],\n",
            "    token_to_idx: dict[TOKEN, int],\n",
            "):\n",
            "    for token, freq in token_freqs.items():\n",
            "        if len(token) < 2:\n",
            "            continue\n",
            "        pairs = get_pairs(token)\n",
            "        for p in pairs:\n",
            "            pair_to_idx[p].add(token_to_idx[token])\n",
            "\n",
            "\n",
            "def get_most_frequent_pair(\n",
            "    pair_freqs: dict[PAIR, int],\n",
            ") -> PAIR:\n",
            "    return max(pair_freqs.keys(), key=lambda k: (pair_freqs[k], k))\n",
            "\n",
            "def update_vocab(\n",
            "    vocab: dict[int, bytes],\n",
            "    new_id: int,\n",
            "    best_pair: PAIR,\n",
            "):\n",
            "    new_vocab = best_pair[0] + best_pair[1]\n",
            "    vocab[new_id] = new_vocab\n",
            "\n",
            "def update_vocab_inverse(\n",
            "    vocab_inverse: dict[bytes, int],\n",
            "    new_id: int,\n",
            "    best_pair: PAIR,\n",
            "):\n",
            "    new_vocab = best_pair[0] + best_pair[1]\n",
            "    vocab_inverse[new_vocab] = new_id\n",
            "\n",
            "def update_merges(\n",
            "    merges: list[PAIR],\n",
            "    best_pair: PAIR,\n",
            "):\n",
            "    merges.append(best_pair)\n",
            "\n",
            "def update_all(\n",
            "    pair_to_idx: dict[PAIR, set[int]],\n",
            "    pair_freqs: dict[PAIR, int],\n",
            "    token_freqs: dict[TOKEN, int],\n",
            "    idx_to_token: dict[int, TOKEN],\n",
            "    best_pair: PAIR,\n",
            "):\n",
            "    affected_idxs = list(pair_to_idx[best_pair])\n",
            "    merged_bytes = best_pair[0] + best_pair[1]\n",
            "\n",
            "    for idx in affected_idxs:\n",
            "        # get new token\n",
            "        i=0\n",
            "        token = idx_to_token[idx]\n",
            "        new_token = []\n",
            "        while(i<(len(token))):\n",
            "            if (i < len(token) - 1) and (token[i] == best_pair[0]) and (token[i+1] == best_pair[1]):\n",
            "                new_token.append(merged_bytes)\n",
            "                i = i + 2\n",
            "            else:\n",
            "                new_token.append(token[i])\n",
            "                i = i + 1\n",
            "        new_token = tuple(new_token)\n",
            "\n",
            "\n",
            "        ## update pair_to_idx\n",
            "        new_pairs = get_pairs(new_token)\n",
            "        affected_pairs = get_pairs(token)\n",
            "        for p in set(new_pairs)-set(affected_pairs):\n",
            "            pair_to_idx[p].add(idx)\n",
            "        for p in set(affected_pairs)-set(new_pairs):\n",
            "            pair_to_idx[p].discard(idx)\n",
            "\n",
            "\n",
            "        ## update pair_freqs\n",
            "        for pair in affected_pairs:\n",
            "            pair_freqs[pair] -= token_freqs[token]\n",
            "        for pair in new_pairs:\n",
            "            pair_freqs[pair] += token_freqs[token]\n",
            "\n",
            "\n",
            "        ## update token_freqs\n",
            "        origin_freq = token_freqs.pop(token)\n",
            "        token_freqs[new_token] = origin_freq\n",
            "\n",
            "        ## update idx_to_token\n",
            "        idx_to_token[idx] = new_token\n",
            "        \n",
            "\n",
            "def train_bpe(\n",
            "    input_path: str,\n",
            "    vocab_size: int,\n",
            "    special_tokens: list[str],\n",
            "    num_processes: int = 4\n",
            "):\n",
            "    print(\"Start Training BPE...\")\n",
            "    start_total = time.time()\n",
            "\n",
            "    print(\"Initialize Vocab and Merges...\")\n",
            "    vocab = {i:bytes([i]) for i in range(256)}\n",
            "    vocab_inverse = {v:k for k,v in vocab.items()}\n",
            "    # Add special tokens\n",
            "    for st in special_tokens:\n",
            "        new_id = len(vocab)\n",
            "        st_bytes = st.encode(\"utf-8\")\n",
            "        vocab[new_id] = st_bytes\n",
            "        vocab_inverse[st_bytes] = new_id\n",
            "    \n",
            "    merges = []\n",
            "\n",
            "    print(\"Calculating chunk boundaries...\")\n",
            "    t0 = time.time()\n",
            "    with open(input_path, 'rb') as f:\n",
            "        # Assuming first special token is the split token\n",
            "        split_token = special_tokens[0].encode(\"utf-8\") if special_tokens else b\"<|endoftext|>\"\n",
            "        boundaries = find_chunk_boundaries(f, num_processes, split_token) # More chunks than processes for load balancing\n",
            "    print(f\"Boundaries calculated in {time.time() - t0:.2f}s\")\n",
            "    \n",
            "    token_freqs = defaultdict(int)\n",
            "\n",
            "    ## Initiate token_freqs\n",
            "    print(\"Update Token Freq (Parallel)...\")\n",
            "    t0 = time.time()\n",
            "    \n",
            "    tasks = []\n",
            "    for i in range(len(boundaries) - 1):\n",
            "        start = boundaries[i]\n",
            "        end = boundaries[i+1]\n",
            "        tasks.append((input_path, start, end, special_tokens, i))\n",
            "    \n",
            "    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
            "        results = list(tqdm(executor.map(process_chunk, tasks), total=len(tasks), desc=\"Token Freqs (Chunks)\", position=0))\n",
            "        \n",
            "        for local_freqs in results:\n",
            "            for token, count in local_freqs.items():\n",
            "                token_freqs[token] += count\n",
            "                \n",
            "    idx_to_token = {i: k for i, k in enumerate(token_freqs.keys())}\n",
            "    token_to_idx = {k: i for i, k in enumerate(token_freqs.keys())}\n",
            "    print(f\"Update Token Freq took {time.time() - t0:.2f}s\")\n",
            "\n",
            "\n",
            "    ## Initiate pair_freqs, pair_to_token, and token_to_pair\n",
            "    print(\"Update Pair Freq...\")\n",
            "    t0 = time.time()\n",
            "    pair_freqs = defaultdict(int)\n",
            "    update_pair_freqs(pair_freqs, token_freqs)\n",
            "    print(f\"Update Pair Freq took {time.time() - t0:.2f}s\")\n",
            "\n",
            "    print(\"Update Pair to idx...\")\n",
            "    t0 = time.time()\n",
            "    pair_to_idx = defaultdict(set)\n",
            "    update_pair2idx(pair_to_idx, token_freqs, token_to_idx)\n",
            "    print(f\"Update Pair to Token took {time.time() - t0:.2f}s\")\n",
            "\n",
            "    print(\"Start Merging...\")\n",
            "    t0 = time.time()\n",
            "    num_merges = vocab_size - 256 - len(special_tokens)\n",
            "    for i in tqdm(range(num_merges), desc=\"Merging\"):\n",
            "        ### find most frequent pair\n",
            "        if not pair_freqs:\n",
            "            break\n",
            "        best_pair = get_most_frequent_pair(pair_freqs)\n",
            "\n",
            "        new_id = len(vocab)\n",
            "\n",
            "        ## update vocab\n",
            "        update_vocab(vocab, new_id, best_pair)\n",
            "\n",
            "        ## update vocab_inverse\n",
            "        update_vocab_inverse(vocab_inverse, new_id, best_pair)\n",
            "\n",
            "        ## update merges\n",
            "        update_merges(merges, best_pair)\n",
            "\n",
            "        update_all(pair_to_idx, pair_freqs, token_freqs, idx_to_token, best_pair)\n",
            "    print(f\"Merging took {time.time() - t0:.2f}s\")\n",
            "    print(f\"Total Training took {time.time() - start_total:.2f}s\")\n",
            "\n",
            "    return vocab, merges"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "b926ac39",
         "metadata": {},
         "source": [
            "#### Prepare"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "71344ec0",
         "metadata": {},
         "outputs": [],
         "source": [
            "test_string = \"abere ererea<|endoftext|>When and where is not as important as who and what. Hi, I am the the Ivan.<|endoftext|> aaa\"\n",
            "# test_string = \"abere ererea\"\n",
            "input_path = \"\"\n",
            "vocab_size = 260\n",
            "special_tokens = [END_TOKEN]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "id": "80e25f26",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'abere ererea'"
                  ]
               },
               "execution_count": 5,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "special_pat = \"|\".join(re.escape(st) for st in special_tokens)\n",
            "segments = re.split(special_pat, test_string)\n",
            "\n",
            "text_segment = segments[0]\n",
            "text_segment"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "3f0e1451",
         "metadata": {},
         "source": [
            "#### Initialize Vocab and Merges"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "id": "0cf7e249",
         "metadata": {},
         "outputs": [],
         "source": [
            "vocab = {i:bytes([i]) for i in range(256)}\n",
            "vocab_inverse = {v:k for k,v in vocab.items()}\n",
            "# Add special tokens\n",
            "for st in special_tokens:\n",
            "    new_id = len(vocab)\n",
            "    st_bytes = st.encode(\"utf-8\")\n",
            "    vocab[new_id] = st_bytes\n",
            "    vocab_inverse[st_bytes] = new_id\n",
            "\n",
            "merges = []"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "49aceb75",
         "metadata": {},
         "source": [
            "#### Update Token Freq"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "id": "ee804c47",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "defaultdict(int,\n",
                     "            {(b'a', b'b', b'e', b'r', b'e'): 1,\n",
                     "             (b' ', b'e', b'r', b'e', b'r', b'e', b'a'): 1})"
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "token_freqs = defaultdict(int)\n",
            "update_token_freqs(token_freqs,text_segment)\n",
            "token_freqs"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "c3561697",
         "metadata": {},
         "source": [
            "#### Update idx freq"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "id": "2c843b71",
         "metadata": {},
         "outputs": [],
         "source": [
            "idx_to_token = {i: k for i, k in enumerate(token_freqs.keys())}\n",
            "token_to_idx = {k: i for i, k in enumerate(token_freqs.keys())}\n",
            "idx_freqs = {token_to_idx[token]: freq for token, freq in token_freqs.items()}"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "0017256a",
         "metadata": {},
         "source": [
            "#### Update Pair Freq"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "id": "824040e0",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "defaultdict(int,\n",
                     "            {(b'a', b'b'): 1,\n",
                     "             (b'b', b'e'): 1,\n",
                     "             (b'e', b'r'): 3,\n",
                     "             (b'r', b'e'): 3,\n",
                     "             (b' ', b'e'): 1,\n",
                     "             (b'e', b'a'): 1})"
                  ]
               },
               "execution_count": 9,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "pair_freqs = defaultdict(int)\n",
            "update_pair_freqs(pair_freqs,token_freqs)\n",
            "pair_freqs"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "9a8ad346",
         "metadata": {},
         "source": [
            "#### Update Pair to idx"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "id": "892a9535",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "defaultdict(set,\n",
                     "            {(b'a', b'b'): {0},\n",
                     "             (b'b', b'e'): {0},\n",
                     "             (b'e', b'r'): {0, 1},\n",
                     "             (b'r', b'e'): {0, 1},\n",
                     "             (b' ', b'e'): {1},\n",
                     "             (b'e', b'a'): {1}})"
                  ]
               },
               "execution_count": 10,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "pair_to_idx = defaultdict(set)\n",
            "update_pair2idx(pair_to_idx, token_freqs, token_to_idx)\n",
            "pair_to_idx"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "fd9d7008",
         "metadata": {},
         "source": [
            "#### Merging"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "id": "99c45417",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "(b'r', b'e')"
                  ]
               },
               "execution_count": 11,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "best_pair = get_most_frequent_pair(pair_freqs)\n",
            "best_pair"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "id": "6d55fb3b",
         "metadata": {},
         "outputs": [],
         "source": [
            "new_id = len(vocab)\n",
            "\n",
            "## update vocab\n",
            "update_vocab(vocab, new_id, best_pair)\n",
            "\n",
            "## update vocab_inverse\n",
            "update_vocab_inverse(vocab_inverse, new_id, best_pair)\n",
            "\n",
            "## update merges\n",
            "update_merges(merges, best_pair)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "id": "b9a1fc7e",
         "metadata": {},
         "outputs": [],
         "source": [
            "affected_idxs = list(pair_to_idx[best_pair])\n",
            "merged_bytes = best_pair[0] + best_pair[1]\n",
            "for idx in affected_idxs:\n",
            "    token = idx_to_token[idx]\n",
            "\n",
            "    i=0\n",
            "    new_token = []\n",
            "    while(i<(len(token))):\n",
            "        if (i < len(token) - 1) and (token[i] == best_pair[0]) and (token[i+1] == best_pair[1]):\n",
            "            new_token.append(merged_bytes)\n",
            "            i = i + 2\n",
            "        else:\n",
            "            new_token.append(token[i])\n",
            "            i = i + 1\n",
            "    new_token = tuple(new_token)\n",
            "\n",
            "    new_pairs = get_pairs(new_token)\n",
            "    affected_pairs = get_pairs(token)\n",
            "    for p in set(new_pairs)-set(affected_pairs):\n",
            "        pair_to_idx[p].add(idx)\n",
            "    for p in set(affected_pairs)-set(new_pairs):\n",
            "        pair_to_idx[p].discard(idx)\n",
            "\n",
            "\n",
            "    for pair in affected_pairs:\n",
            "        pair_freqs[pair] -= idx_freqs[idx]\n",
            "    for pair in new_pairs:\n",
            "        pair_freqs[pair] += idx_freqs[idx]\n",
            "\n",
            "    # origin_freq = token_freqs.pop(token)\n",
            "    # token_freqs[new_token] = origin_freq\n",
            "\n",
            "    idx_to_token[idx] = new_token"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "id": "6622a581",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "defaultdict(set,\n",
                     "            {(b'a', b'b'): {0},\n",
                     "             (b'b', b'e'): {0},\n",
                     "             (b'e', b'r'): set(),\n",
                     "             (b'r', b'e'): set(),\n",
                     "             (b' ', b'e'): {1},\n",
                     "             (b'e', b'a'): set(),\n",
                     "             (b'e', b're'): {0, 1},\n",
                     "             (b're', b'a'): {1},\n",
                     "             (b're', b're'): {1}})"
                  ]
               },
               "execution_count": 18,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "pair_to_idx"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "id": "1c8bc38a",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "defaultdict(int,\n",
                     "            {(b'a', b'b'): 1,\n",
                     "             (b'b', b'e'): 1,\n",
                     "             (b'e', b'r'): 0,\n",
                     "             (b'r', b'e'): 0,\n",
                     "             (b' ', b'e'): 1,\n",
                     "             (b'e', b'a'): 0,\n",
                     "             (b'e', b're'): 2,\n",
                     "             (b're', b're'): 1,\n",
                     "             (b're', b'a'): 1})"
                  ]
               },
               "execution_count": 19,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "pair_freqs"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 86,
         "id": "a4f47552",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{0: 1, 1: 1}"
                  ]
               },
               "execution_count": 86,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "idx_freqs"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 46,
         "id": "bcb800c0",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{0: (b'a', b'b', b'e', b're'), 1: (b' ', b'e', b're', b're', b'a')}"
                  ]
               },
               "execution_count": 46,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "idx_to_token"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "ca3e2ea9",
         "metadata": {},
         "source": [
            "### 2.6 BPE Tokenizer: Encoding and Decoding"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "1c3506f3",
         "metadata": {},
         "source": [
            "##### 2.6.1 Encoding text"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "8ac9f7ff",
         "metadata": {},
         "source": [
            "##### 2.6.2 Decoding text"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "id": "9d83831d",
         "metadata": {},
         "outputs": [],
         "source": [
            "test_string = \"abere ererea<|endoftext|>When and where is not as important as who and what. Hi, I am the the Ivan.<|endoftext|> aaa\"\n",
            "\n",
            "special_tokens = [END_TOKEN]\n",
            "vocab_filepath = f\"/Users/yifanli/Github/stanford-cs336/stanford-cs336-assignment1-basics/outputs/TinyStoriesV2-GPT4-train-vocab-10000.pkl\"\n",
            "merges_filepath = f\"/Users/yifanli/Github/stanford-cs336/stanford-cs336-assignment1-basics/outputs/TinyStoriesV2-GPT4-train-merge-10000.pkl\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "id": "611c7faa",
         "metadata": {},
         "outputs": [],
         "source": [
            "with open(vocab_filepath, 'rb') as f:\n",
            "    vocab = pickle.load(f)\n",
            "with open(merges_filepath, 'rb') as f:\n",
            "    merges = pickle.load(f)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 22,
         "id": "02db0369",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'abere ererea'"
                  ]
               },
               "execution_count": 22,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "special_pat = \"|\".join(re.escape(st) for st in special_tokens)\n",
            "segments = re.split(special_pat, test_string)\n",
            "\n",
            "text_segment = segments[0]\n",
            "text_segment"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 75,
         "id": "b7f5e84c",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[b' ', b'e', b'r', b'e', b'r', b'e', b'a']"
                  ]
               },
               "execution_count": 75,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "token = ' ererea'\n",
            "token_bytes = list(bytes([b]) for b in token.encode(\"utf-8\"))\n",
            "token_bytes"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 76,
         "id": "7cf0c06a",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "b'r' b'e'\n",
                  "b'r' b'e'\n",
                  "b' ' b'e'\n"
               ]
            }
         ],
         "source": [
            "while 1:\n",
            "    merge_position = -1\n",
            "    smallest_rank = len(merges)\n",
            "    for i in range(len(token_bytes)-1):\n",
            "        current_pair = (token_bytes[i], token_bytes[i+1])\n",
            "        rank = merges_to_rank.get(current_pair, -1)\n",
            "        if rank == -1:\n",
            "            continue\n",
            "        if rank < smallest_rank:\n",
            "            smallest_rank = rank\n",
            "            merge_position = i\n",
            "    if merge_position == -1:\n",
            "        break\n",
            "    print(token_bytes[merge_position], token_bytes[merge_position+1])\n",
            "    token_bytes = token_bytes[:(merge_position)] + [token_bytes[merge_position]+token_bytes[merge_position+1]]+token_bytes[(merge_position+2):]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 87,
         "id": "4c45ad2f",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "array([345, 273, 273,  97], dtype=uint16)"
                  ]
               },
               "execution_count": 87,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "encoded_token = []\n",
            "for i in token_bytes:\n",
            "    encoded_token.append(vocab_inverse.get(i, \"aaa\"))\n",
            "np.array(encoded_token, dtype=np.uint16)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 35,
         "id": "c2a85b24",
         "metadata": {},
         "outputs": [],
         "source": [
            "class Tokenizer():\n",
            "    def __init__(\n",
            "        self, \n",
            "        vocab: dict[int, bytes], \n",
            "        merges: list[PAIR],\n",
            "        special_tokens: list[str] | None = None,\n",
            "    ):\n",
            "        self.vocab = vocab\n",
            "        self.merges = merges\n",
            "\n",
            "        if special_tokens:\n",
            "            new_index = len(vocab)\n",
            "            for st in special_tokens:\n",
            "                vocab[new_index] = st\n",
            "                new_index += 1\n",
            "            self.special_tokens =  sorted([END_TOKEN]+special_tokens, key=len, reverse=True)\n",
            "        else:\n",
            "            self.special_tokens = [END_TOKEN]\n",
            "        \n",
            "        self.vocab_inverse = {v: k for k, v in vocab.items()}\n",
            "        self.merges_to_rank = {m: i for i, m in enumerate(merges)}\n",
            "\n",
            "\n",
            "    @classmethod\n",
            "    def from_files(\n",
            "        cls, \n",
            "        vocab_filepath: str, \n",
            "        merges_filepath: str, \n",
            "        special_tokens: list[str] | None = None,\n",
            "    ) :\n",
            "        with open(vocab_filepath, \"rb\") as f:\n",
            "            vocab = pickle.load(f)\n",
            "        \n",
            "        with open(merges_filepath, \"rb\") as f:\n",
            "            merges = pickle.load(f)\n",
            "        return cls(vocab, merges, special_tokens=special_tokens)\n",
            "\n",
            "\n",
            "    def merge_tokens(self, token_bytes: list[bytes]) -> list[bytes]:\n",
            "        while 1:\n",
            "            merge_position = -1\n",
            "            smallest_rank = len(self.merges)\n",
            "            for i in range(len(token_bytes)-1):\n",
            "                current_pair = (token_bytes[i], token_bytes[i+1])\n",
            "                rank = self.merges_to_rank.get(current_pair, -1)\n",
            "                if rank == -1:\n",
            "                    continue\n",
            "                if rank < smallest_rank:\n",
            "                    smallest_rank = rank\n",
            "                    merge_position = i\n",
            "            if merge_position == -1:\n",
            "                break\n",
            "            token_bytes = token_bytes[:(merge_position)] + [token_bytes[merge_position]+token_bytes[merge_position+1]]+token_bytes[(merge_position+2):]\n",
            "        return token_bytes\n",
            "\n",
            "    def encode(self, text: str) -> list[int]:\n",
            "        token_bytes = list(bytes([b]) for b in text.encode(\"utf-8\"))\n",
            "        token_bytes = self.merge_tokens(token_bytes)\n",
            "        encoded_token = []\n",
            "        for i in token_bytes:\n",
            "            encoded_token.append(self.vocab_inverse.get(i, \"aaa\"))\n",
            "        return encoded_token\n",
            "\n",
            "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
            "        pass\n",
            "\n",
            "    def decode(self, ids: list[int]) -> str:\n",
            "        unk_bytes = '\\ufffd'.encode('utf-8')\n",
            "        bytes_list = [self.vocab.get(i, unk_bytes) for i in ids]\n",
            "        return b\"\".join(bytes_list).decode(\"utf-8\", errors=\"replace\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 36,
         "id": "96a520b3",
         "metadata": {},
         "outputs": [],
         "source": [
            "tokenizer = Tokenizer(vocab, merges)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 37,
         "id": "c4367bf0",
         "metadata": {},
         "outputs": [],
         "source": [
            "tokenizer = Tokenizer.from_files(vocab_filepath=vocab_filepath, merges_filepath=merges_filepath)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 38,
         "id": "53af4d18",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[1183, 44, 1569, 33]"
                  ]
               },
               "execution_count": 38,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "ids = tokenizer.encode('Hello, world!')\n",
            "ids"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 39,
         "id": "3f4a8000",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'Hello, world!'"
                  ]
               },
               "execution_count": 39,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "segment = tokenizer.decode(ids)\n",
            "segment"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "ff49ad7c",
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.12"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
